# T-JEPA
A take on decoder only auto-regressive JEPA architecture.
# **T-decoder-JEPA: Integrating Temporal Joint Embedding Prediction into Decoder-Only Language Models**

Abstract

Decoder-only transformer models, pre-trained with causal language modeling (LM) objectives, have demonstrated remarkable capabilities. However, their reliance solely on predicting the immediate next token might limit the depth of semantic and structural understanding required for complex reasoning tasks. To address this, we propose T-decoder-JEPA, a novel architecture that integrates principles from the Joint Embedding Predictive Architecture (JEPA) into a standard decoder-only framework. T-decoder-JEPA augments the causal LM objective with a JEPA-inspired self-supervised task: predicting the representations of masked future segments (targets) using representations from an unmasked causal past (context). Crucially, the target representations are generated by a non-causal, exponential moving average (EMA) copy of the decoder backbone, providing a rich supervisory signal representing the model's accumulated "experience" applied to the full context. The prediction occurs in embedding space via a dedicated predictor module that leverages both self-attention and cross-attention to the causal backbone's outputs. This multi-task learning setup encourages the backbone decoder to learn richer, more predictive representations that capture longer-range dependencies and abstract structural information, potentially enhancing performance on downstream tasks requiring deep reasoning, such as mathematical problem-solving.

1. Introduction

Large Language Models (LLMs) based on the decoder-only transformer architecture (e.g., GPT series [Radford et al., 2018, 2019; Brown et al., 2020], LLaMA [Touvron et al., 2023]) have achieved state-of-the-art performance across a wide range of natural language processing tasks. Their core training objective, causal language modeling, involves predicting the next token in a sequence given the preceding tokens. While effective, this objective primarily focuses on local dependencies and statistical co-occurrence, which may not be sufficient to foster the deep understanding of structure, causality, and long-range relationships needed for complex reasoning tasks [Mallen et al., 2023].

Self-supervised learning (SSL) beyond simple next-token prediction offers a promising avenue to improve the representational quality of language models. Masked language modeling (MLM) [Devlin et al., 2019] and other reconstruction-based objectives have proven successful, particularly for encoder-based models. Recently, the Joint Embedding Predictive Architecture (JEPA) [LeCun, 2022; Assran et al., 2023] has emerged as a powerful SSL paradigm, particularly in computer vision (I-JEPA [Assran et al., 2023]). JEPA aims to learn abstract representations by predicting the representations of masked portions of the input (targets) from unmasked portions (context), operating entirely within the embedding space. This avoids the computational burden and potential semantic limitations of predicting raw pixels or tokens.

Inspired by JEPA's success, we propose T-decoder-JEPA (Temporal Decoder Joint Embedding Predictive Architecture), a novel approach to integrate JEPA principles directly into a decoder-only transformer. Our key contributions are:

1.  Novel Architecture: We present a decoder-only transformer enhanced with a JEPA-based self-supervised objective, operating alongside the standard causal LM task.
2.  Causal/Non-Causal Mechanism: We introduce a specific mechanism where target representations for the JEPA task are derived from a non-causal pass through an EMA target encoder, while the prediction is made using information from the standard causal backbone pass.
3.  Predictor Design: We detail a predictor module employing causal self-attention and cross-attention to the backbone decoder's states, enabling effective integration of context information for target prediction.
4.  Multi-Task Formulation: We combine the JEPA prediction loss (MSE in embedding space) with the causal LM loss (Cross-Entropy), allowing the model to benefit from both objectives simultaneously.

We hypothesize that T-decoder-JEPA encourages the learning of more robust and predictive representations, better suited for tasks requiring multi-step reasoning, as exemplified by mathematical datasets like GSM8K [Cobbe et al., 2021].

2. Related Work

*   Decoder-Only Language Models: Our work builds upon the standard decoder-only architecture [Vaswani et al., 2017] popularized by the GPT models [Radford et al., 2018, 2019; Brown et al., 2020]. These models are typically trained autoregressively using the causal LM objective. We retain this objective but augment it with our JEPA task.
*   Self-Supervised Learning in NLP: SSL has been pivotal in NLP. BERT [Devlin et al., 2019] introduced Masked Language Modeling (MLM) for bidirectional encoders. Denoising autoencoders [Vincent et al., 2008] and contrastive methods [Logeswaran & Lee, 2018; Gao et al., 2021] are other prominent approaches. T-decoder-JEPA differs by predicting representations in embedding space rather than reconstructing tokens (like MLM) or using contrastive losses.
*   Joint Embedding Predictive Architectures (JEPA): JEPA [LeCun, 2022] proposes learning predictive world models. I-JEPA [Assran et al., 2023] successfully applied this to vision, demonstrating strong performance by predicting representations of masked image patches from context patches using an EMA target encoder. Our work adapts this core idea to the sequential, temporal nature of language within a decoder framework.
*   Multi-Task Learning (MTL) in NLP: Combining different objectives is common in NLP [Caruana, 1997; Raffel et al., 2020]. T-decoder-JEPA employs MTL by combining the JEPA loss and the LM loss, aiming for synergistic benefits where the JEPA task regularizes and enriches the representations learned primarily for the LM task.
*   Reasoning in Language Models: Improving the reasoning capabilities of LLMs is an active research area [Wei et al., 2022; Nye et al., 2021]. While some approaches focus on fine-tuning or prompting techniques (e.g., Chain-of-Thought [Wei et al., 2022]), T-decoder-JEPA aims to enhance the foundational representational capacity of the pre-trained model itself through its novel training objectives.

3. Methodology: T-decoder-JEPA

The T-decoder-JEPA architecture integrates a JEPA prediction task into a standard decoder-only transformer backbone trained with a causal LM objective. The key components are the Backbone Decoder, the Target Encoder, the Span Selection Strategy, the Predictor, and the combined Loss Function.

3.1. Backbone Decoder (Causal)

The core of the model is a standard multi-layer Transformer decoder, denoted `f_θ`. It processes an input sequence `X = (x_1, ..., x_T)` autoregressively.
*   Input: Token sequence `X`.
*   Processing: Standard decoder blocks with causal self-attention (e.g., using RoPE [Su et al., 2024] for positional information) and feed-forward layers. Layer Normalization (Pre-LN [Ba et al., 2016; Xiong et al., 2020]) is used.
*   Output: A sequence of hidden states `H_c = (h_{c,1}, ..., h_{c,T}) = f_θ(X)`, where each `h_{c,t}` depends only on `x_1, ..., x_t`.
*   Role:
    1.  Generates representations for the standard causal LM loss.
    2.  Provides context representations (via cross-attention keys/values) to the Predictor.

3.2. Target Encoder (Non-Causal EMA)

The Target Encoder, denoted `f_θ'`, is structurally identical to the Backbone Decoder but its parameters `θ'` are an Exponential Moving Average (EMA) of the backbone parameters `θ`: `θ' ← α θ' + (1 - α) θ`. Crucially, it processes the input sequence non-causally.
*   Input: Token sequence `X`.
*   Processing: Identical transformer blocks as the backbone, but the self-attention mechanism is configured to be non-causal (bi-directional), allowing each position to attend to all other positions in the sequence (respecting padding).
*   Output: A sequence of hidden states `H_{nc} = (h_{nc,1}, ..., h_{nc,T}) = f_θ'(X)`, where each `h_{nc,t}` depends on the entire sequence `x_1, ..., x_T`.
*   Role: Generates the target representations for the JEPA prediction task. Its parameters are not updated via backpropagation; only through EMA updates from `f_θ`. These target representations reflect the model's accumulated "memory" or "experience" (`θ'`) applied to understand the full context of the current sequence `X`.

Analogy: The Detective and the Full Reader: To build intuition for the causal/non-causal dynamic, consider the causal Backbone Decoder (`f_θ`) as a detective reading a novel one page at a time. When encountering missing pages (target spans), the detective can only guess their content based on what has been read so far (the causal context `H_c`). The non-causal Target Encoder (`f_θ'`) is like someone who has already read the entire book; they possess the complete understanding and "memory" of how those missing pages fit into the overall narrative. The output `H_{nc}` represents this perfect understanding, serving as the ground truth for what the detective should infer.

3.3. Span Selection and Masking

For the JEPA task, we sample multiple target spans within each sequence `X`.
*   A proportion of the sequence is designated as the JEPA context.
*   Several non-overlapping target spans `s_i = (start_i, end_i)` are randomly selected from the remaining portion.
*   Parameters control the number of target spans, minimum/maximum span length, and context/target ratios.
*   A `context_mask` indicates which positions belong to the context (1) and which belong to targets or padding (0).
*   An `attention_mask` indicates padding tokens (0) vs. real tokens (1).

3.4. Predictor

The Predictor, `g_φ`, is another multi-layer Transformer-based module responsible for predicting the target representations.
*   Input: A modified sequence where context positions retain their original embeddings (or potentially representations from `H_c`), while target positions are replaced with learnable `[MASK]` tokens/embeddings. Let this input be `X_masked`.
*   Processing: The predictor consists of blocks performing:
    1.  Causal Self-Attention: Operates on the `X_masked` sequence representation, using the `attention_mask` for padding but maintaining causality.
    2.  Cross-Attention: Attends to the output states `H_c` from the causal Backbone Decoder. Queries come from the predictor's self-attention output. Keys and Values come from `H_c`. The `context_mask` is used here to ensure the predictor only attends to keys/values corresponding to the context positions in the backbone output.
    3.  Feed-Forward Layers.
*   Output: Predicted representations `P = (p_1, ..., p_T) = g_φ(X_masked, H_c, context_mask)`. We are interested in the outputs corresponding to the target spans: `P[s_i] = (p_{start_i}, ..., p_{end_i})`.
*   Role: Learns to predict the non-causal target representations `H_{nc}[s_i]` using only the masked input and the causal context information from the backbone `H_c`. It acts like the detective's assistant, trying to reconstruct the meaning of the missing pages using only the clues gathered so far.

3.5. Loss Functions

The model is trained with a combined loss:
`L_total = L_JEPA + λ * L_LM`

*   JEPA Loss (`L_JEPA`): The Mean Squared Error (MSE) between the predictor's output embeddings and the target encoder's output embeddings for all target spans `s_i`:
    `L_JEPA = (1 / N_spans) * Σ_{i} || P[s_i] - H_{nc}[s_i] ||^2`
    (Normalization by span length or averaging per-span loss might also be considered). The target `H_{nc}[s_i]` is detached from the computation graph. This loss measures the difference between the assistant's guess and the full reader's knowledge.
*   LM Loss (`L_LM`): The standard causal language modeling loss (Cross-Entropy) calculated using the output of the causal Backbone Decoder `H_c` and its associated LM head (which typically shares weights with the input embedding layer):
    `L_LM = CrossEntropy(LM_Head(H_c), X_shifted)`
*   `λ` is a hyperparameter balancing the two loss terms.

3.6. Training

Training proceeds by minimizing `L_total`. Parameters `θ` (backbone) and `φ` (predictor) are updated via gradient descent (e.g., AdamW [Loshchilov & Hutter, 2019]). Target encoder parameters `θ'` are updated using EMA after each optimizer step on `θ`.

3.7. Mathematical Formulation

To formalize the process during training and inference:

Notation:
*   `X = (x_1, ..., x_T)`: Input token sequence.
*   `θ, θ', φ`: Parameters for backbone, target encoder (EMA of θ), and predictor.
*   `f_θ`: Causal backbone decoder function.
*   `f_θ'`: Non-causal target encoder function.
*   `g_φ`: Predictor function.
*   `LM_Head`: Language modeling head (linear layer).
*   `H_c`: Hidden states from causal backbone. `H_c[t]` depends on `X_{1:t}`.
*   `H_{nc}`: Hidden states from non-causal target encoder. `H_{nc}[t]` depends on `X_{1:T}`.
*   `s = (start, end)`: Indices defining a target span.
*   `X_masked`: Input to predictor with target spans masked.
*   `Context(s)`: Indices of context tokens relevant for predicting span `s`.
*   `P[s]`: Predicted representation for span `s`.
*   `CE`: Cross-Entropy loss. `MSE`: Mean Squared Error loss. `detach`: Stop gradient.
*   `α`: EMA decay rate. `λ`: LM loss weight. `∇`: Gradient. `LR`: Learning rate.
*   `temp`: Temperature for sampling. `TopP`: Top-p sampling function.

Training Step:
1.  Causal Pass (Backbone): `H_c = f_θ(X)`
2.  Non-Causal Pass (Target Encoder): `H_{nc} = f_θ'(X)` (with `torch.no_grad()`)
3.  Prediction (Predictor): `P = g_φ(X_masked, H_c[Context(s)])` for all target spans `s`.
4.  LM Loss: `L_LM = CE(LM_Head(H_c[:, :-1]), X[:, 1:])` (ignoring padding)
5.  JEPA Loss: `L_JEPA = mean_s( MSE(P[s], detach(H_{nc}[s])) )`
6.  Total Loss: `L_total = L_JEPA + λ * L_LM`
7.  Parameter Update:
    *   `θ ← θ - LR * ∇_θ L_total`
    *   `φ ← φ - LR * ∇_φ L_total`
    *   `θ' ← α * θ' + (1 - α) * θ`

Inference (Autoregressive Generation):
Given context `X_context = (x_1, ..., x_k)`:
For `t` from `k` to `max_length - 1`:
1.  Get Last Causal State: `h_{c,t} = f_θ(X_context)_{last_token_state}`
2.  Get Logits: `logits_t = LM_Head(h_{c,t})`
3.  Sample Next Token:
    *   `probs_t = Softmax(TopP(logits_t / temp))`
    *   `x_{t+1} ~ Multinomial(probs_t)`
4.  Append: `X_context ← Append(X_context, x_{t+1})`
Return `X_context`.
*(Note: Only the causal backbone `f_θ` and `LM_Head` are used during inference)*.

4. Experiments (Planned)

*   Dataset: We plan to evaluate T-decoder-JEPA primarily on the GSM8K dataset [Cobbe et al., 2021], a collection of grade-school math word problems requiring multi-step reasoning.
*   Baselines:
    1.  A standard decoder-only model of identical size and configuration, trained solely with the causal LM objective (`λ=0`).
    2.  (Optional) Other relevant SSL methods adapted to the decoder framework, if feasible.
*   Evaluation Metrics:
    1.  Accuracy on the GSM8K test set (final answer matching).
    2.  Perplexity on a standard language modeling benchmark (e.g., WikiText) to assess the impact on general language modeling capabilities.
    3.  JEPA prediction loss during training as a diagnostic metric.
*   Ablation Studies: We will investigate the contribution of key components:
    1.  Effect of the JEPA loss (varying `λ`).
    2.  Importance of the non-causal target encoder (vs. using causal targets).
    3.  Impact of the EMA update.
    4.  Sensitivity to predictor depth and architecture.
    5.  Influence of span selection hyperparameters.
*   Hypothesized Results: We expect T-decoder-JEPA to achieve higher accuracy on GSM8K compared to the LM-only baseline, demonstrating improved reasoning capabilities stemming from the enriched representations learned via the JEPA objective. We anticipate that general LM performance (perplexity) will remain competitive or potentially improve slightly due to the regularizing effect of the JEPA task.

(Table 1: Placeholder for GSM8K accuracy results for T-decoder-JEPA vs. baselines.)
(Table 2: Placeholder for Perplexity results.)

5. Discussion

The core hypothesis behind T-decoder-JEPA is that forcing the model to predict future representations (derived non-causally) from a causal context compels it to learn more abstract and robust features. The non-causal target `H_{nc}` provides a rich signal about the complete sequence structure, which the predictor `g_φ` must learn to anticipate based only on the causally-generated context `H_c`. This implicit "lookahead" in the embedding space may encourage the backbone `f_θ` to encode information about long-range dependencies and underlying semantic/logical structure more effectively than relying solely on next-token prediction.

Analogy: The Student and Teacher: The learning dynamic can be likened to training a student (the predictor `g_φ`) to predict the ending (target representation `H_{nc}[s]`) of a story by only reading the beginning (causal context representations `H_c`). The teacher (the non-causal target encoder `f_θ'`) knows the full story and provides the perfect target representation. The student makes a guess (`P[s]`), and the difference (JEPA loss) signals the error. This error signal not only improves the student (`g_φ`) but crucially puts pressure back on the causal decoder (`f_θ`)—the provider of the initial information—to generate richer, more predictive "hints" (the hidden states `H_c`) in the first place. Over time, the causal decoder learns to encode predictive signals about the future within its current state, even though it cannot see the future directly.

Implications:
*   Enhanced Representations: The JEPA task acts as a regularizer, potentially leading to representations less sensitive to superficial correlations and more attuned to deeper structure. The causal hidden states `H_c` become implicitly conditioned to support future prediction.
*   Improved Reasoning & Downstream Tasks: By learning to anticipate representations related to future steps or outcomes, the model might develop capabilities more aligned with planning and multi-step reasoning. This should improve performance on downstream tasks like generation or classification that rely on these enriched causal states.

Limitations:
*   Computational Cost: The architecture involves forward passes through the backbone, target encoder, and predictor, increasing computational requirements compared to standard LM training.
*   Hyperparameter Sensitivity: Performance might be sensitive to the choice of span selection strategy, the weighting factor `λ`, EMA decay `α`, and predictor architecture.
*   Complexity: The interplay between the causal backbone, non-causal target, and predictor introduces complexity in analysis and debugging.

Future Work:
*   Scaling T-decoder-JEPA to larger models and datasets.
*   Applying the architecture to other domains requiring long-range understanding or planning.
*   In-depth analysis of the learned representations to understand the effects of the JEPA objective.
*   Exploring alternative predictor designs or target generation strategies.

6. Conclusion

We introduced T-decoder-JEPA, a novel architecture that integrates the principles of Joint Embedding Predictive Architecture into a decoder-only transformer. By augmenting the standard causal language modeling objective with a task that predicts non-causal future representations from a causal context—using an EMA target encoder for ground truth and a dedicated predictor—T-decoder-JEPA aims to learn richer, more predictive internal representations. The training dynamic encourages the causal backbone to generate hidden states that are implicitly predictive of future sequence structure. We hypothesize this approach will enhance the capabilities of decoder-only models on complex reasoning tasks. Planned experiments on datasets like GSM8K will evaluate the effectiveness of this architecture compared to standard LM baselines. T-decoder-JEPA offers a promising direction for improving the representational power and reasoning abilities of large language models through principled self-supervised learning.

7. References

[Assran et al., 2023] Assran, M., et al. (2023). Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture. arXiv preprint arXiv:2301.08243.
[Ba et al., 2016] Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016). Layer normalization. arXiv preprint arXiv:1607.06450.
[Brown et al., 2020] Brown, T. B., et al. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33, 1877-1901.
[Caruana, 1997] Caruana, R. (1997). Multitask learning. Machine learning, 28(1), 41-75.
[Cobbe et al., 2021] Cobbe, K., et al. (2021). Training Verifiers to Solve Math Word Problems. arXiv preprint arXiv:2110.14168.
[Devlin et al., 2019] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). Bert: Pre-training of deep bidirectional transformers for language understanding. Proceedings of NAACL-HLT 2019.
[Gao et al., 2021] Gao, T., Yao, X., & Chen, D. (2021). SimCSE: Simple Contrastive Learning of Sentence Embeddings. arXiv preprint arXiv:2104.08821.
[LeCun, 2022] LeCun, Y. (2022). A Path Towards Autonomous Machine Intelligence. OpenReview.
[Logeswaran & Lee, 2018] Logeswaran, L., & Lee, H. (2018). An efficient framework for learning sentence representations. arXiv preprint arXiv:1803.02893.
[Loshchilov & Hutter, 2019] Loshchilov, I., & Hutter, F. (2019). Decoupled weight decay regularization. ICLR 2019.
[Mallen et al., 2023] Mallen, A., et al. (2023). When Do Pre-Training Objectives Help? An Empirical Study of Multi-Task Transfer Learning. arXiv preprint arXiv:2304.14748.
[Nye et al., 2021] Nye, M., et al. (2021). Show Your Work: Scratchpads for Intermediate Computation with Language Models. arXiv preprint arXiv:2112.00114.
[Radford et al., 2018] Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language understanding by generative pre-training. OpenAI Blog.
[Radford et al., 2019] Radford, A., et al. (2019). Language models are unsupervised multitask learners. OpenAI Blog, 1(8), 9.
[Raffel et al., 2020] Raffel, C., et al. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140), 1-67.
[Su et al., 2024] Su, J., et al. (2024). Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568, 127063. (Note: Original RoPE work might be cited from earlier arXiv versions if preferred).
[Touvron et al., 2023] Touvron, H., et al. (2023). Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.
[Vaswani et al., 2017] Vaswani, A., et al. (2017). Attention is all you need. Advances in neural information processing systems, 30.
[Vincent et al., 2008] Vincent, P., Larochelle, H., Bengio, Y., & Manzagol, P. A. (2008). Extracting and composing robust features with denoising autoencoders. ICML 2008.
[Wei et al., 2022] Wei, J., et al. (2022). Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35, 24824-24837.
[Xiong et al., 2020] Xiong, R., et al. (2020). On layer normalization in the transformer architecture. ICML 2020.
