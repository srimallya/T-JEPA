{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "132ASPDFDGX4MPfXggnPPUzeWITmLW8y2",
      "authorship_tag": "ABX9TyOWu+CgHqgLBELAlg+Q+HiM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srimallya/T-JEPA/blob/main/t_jepa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **T-decoder-JEPA: Integrating Temporal Joint Embedding Prediction into Decoder-Only Language Models**\n",
        "\n",
        "Abstract\n",
        "\n",
        "Decoder-only transformer models, pre-trained with causal language modeling (LM) objectives, have demonstrated remarkable capabilities. However, their reliance solely on predicting the immediate next token might limit the depth of semantic and structural understanding required for complex reasoning tasks. To address this, we propose T-decoder-JEPA, a novel architecture that integrates principles from the Joint Embedding Predictive Architecture (JEPA) into a standard decoder-only framework. T-decoder-JEPA augments the causal LM objective with a JEPA-inspired self-supervised task: predicting the representations of masked future segments (targets) using representations from an unmasked causal past (context). Crucially, the target representations are generated by a non-causal, exponential moving average (EMA) copy of the decoder backbone, providing a rich supervisory signal representing the model's accumulated \"experience\" applied to the full context. The prediction occurs in embedding space via a dedicated predictor module that leverages both self-attention and cross-attention to the causal backbone's outputs. This multi-task learning setup encourages the backbone decoder to learn richer, more predictive representations that capture longer-range dependencies and abstract structural information, potentially enhancing performance on downstream tasks requiring deep reasoning, such as mathematical problem-solving.\n",
        "\n",
        "1. Introduction\n",
        "\n",
        "Large Language Models (LLMs) based on the decoder-only transformer architecture (e.g., GPT series [Radford et al., 2018, 2019; Brown et al., 2020], LLaMA [Touvron et al., 2023]) have achieved state-of-the-art performance across a wide range of natural language processing tasks. Their core training objective, causal language modeling, involves predicting the next token in a sequence given the preceding tokens. While effective, this objective primarily focuses on local dependencies and statistical co-occurrence, which may not be sufficient to foster the deep understanding of structure, causality, and long-range relationships needed for complex reasoning tasks [Mallen et al., 2023].\n",
        "\n",
        "Self-supervised learning (SSL) beyond simple next-token prediction offers a promising avenue to improve the representational quality of language models. Masked language modeling (MLM) [Devlin et al., 2019] and other reconstruction-based objectives have proven successful, particularly for encoder-based models. Recently, the Joint Embedding Predictive Architecture (JEPA) [LeCun, 2022; Assran et al., 2023] has emerged as a powerful SSL paradigm, particularly in computer vision (I-JEPA [Assran et al., 2023]). JEPA aims to learn abstract representations by predicting the representations of masked portions of the input (targets) from unmasked portions (context), operating entirely within the embedding space. This avoids the computational burden and potential semantic limitations of predicting raw pixels or tokens.\n",
        "\n",
        "Inspired by JEPA's success, we propose T-decoder-JEPA (Temporal Decoder Joint Embedding Predictive Architecture), a novel approach to integrate JEPA principles directly into a decoder-only transformer. Our key contributions are:\n",
        "\n",
        "1.  Novel Architecture: We present a decoder-only transformer enhanced with a JEPA-based self-supervised objective, operating alongside the standard causal LM task.\n",
        "2.  Causal/Non-Causal Mechanism: We introduce a specific mechanism where target representations for the JEPA task are derived from a non-causal pass through an EMA target encoder, while the prediction is made using information from the standard causal backbone pass.\n",
        "3.  Predictor Design: We detail a predictor module employing causal self-attention and cross-attention to the backbone decoder's states, enabling effective integration of context information for target prediction.\n",
        "4.  Multi-Task Formulation: We combine the JEPA prediction loss (MSE in embedding space) with the causal LM loss (Cross-Entropy), allowing the model to benefit from both objectives simultaneously.\n",
        "\n",
        "We hypothesize that T-decoder-JEPA encourages the learning of more robust and predictive representations, better suited for tasks requiring multi-step reasoning, as exemplified by mathematical datasets like GSM8K [Cobbe et al., 2021].\n",
        "\n",
        "2. Related Work\n",
        "\n",
        "*   Decoder-Only Language Models: Our work builds upon the standard decoder-only architecture [Vaswani et al., 2017] popularized by the GPT models [Radford et al., 2018, 2019; Brown et al., 2020]. These models are typically trained autoregressively using the causal LM objective. We retain this objective but augment it with our JEPA task.\n",
        "*   Self-Supervised Learning in NLP: SSL has been pivotal in NLP. BERT [Devlin et al., 2019] introduced Masked Language Modeling (MLM) for bidirectional encoders. Denoising autoencoders [Vincent et al., 2008] and contrastive methods [Logeswaran & Lee, 2018; Gao et al., 2021] are other prominent approaches. T-decoder-JEPA differs by predicting representations in embedding space rather than reconstructing tokens (like MLM) or using contrastive losses.\n",
        "*   Joint Embedding Predictive Architectures (JEPA): JEPA [LeCun, 2022] proposes learning predictive world models. I-JEPA [Assran et al., 2023] successfully applied this to vision, demonstrating strong performance by predicting representations of masked image patches from context patches using an EMA target encoder. Our work adapts this core idea to the sequential, temporal nature of language within a decoder framework.\n",
        "*   Multi-Task Learning (MTL) in NLP: Combining different objectives is common in NLP [Caruana, 1997; Raffel et al., 2020]. T-decoder-JEPA employs MTL by combining the JEPA loss and the LM loss, aiming for synergistic benefits where the JEPA task regularizes and enriches the representations learned primarily for the LM task.\n",
        "*   Reasoning in Language Models: Improving the reasoning capabilities of LLMs is an active research area [Wei et al., 2022; Nye et al., 2021]. While some approaches focus on fine-tuning or prompting techniques (e.g., Chain-of-Thought [Wei et al., 2022]), T-decoder-JEPA aims to enhance the foundational representational capacity of the pre-trained model itself through its novel training objectives.\n",
        "\n",
        "3. Methodology: T-decoder-JEPA\n",
        "\n",
        "The T-decoder-JEPA architecture integrates a JEPA prediction task into a standard decoder-only transformer backbone trained with a causal LM objective. The key components are the Backbone Decoder, the Target Encoder, the Span Selection Strategy, the Predictor, and the combined Loss Function.\n",
        "\n",
        "3.1. Backbone Decoder (Causal)\n",
        "\n",
        "The core of the model is a standard multi-layer Transformer decoder, denoted `f_θ`. It processes an input sequence `X = (x_1, ..., x_T)` autoregressively.\n",
        "*   Input: Token sequence `X`.\n",
        "*   Processing: Standard decoder blocks with causal self-attention (e.g., using RoPE [Su et al., 2024] for positional information) and feed-forward layers. Layer Normalization (Pre-LN [Ba et al., 2016; Xiong et al., 2020]) is used.\n",
        "*   Output: A sequence of hidden states `H_c = (h_{c,1}, ..., h_{c,T}) = f_θ(X)`, where each `h_{c,t}` depends only on `x_1, ..., x_t`.\n",
        "*   Role:\n",
        "    1.  Generates representations for the standard causal LM loss.\n",
        "    2.  Provides context representations (via cross-attention keys/values) to the Predictor.\n",
        "\n",
        "3.2. Target Encoder (Non-Causal EMA)\n",
        "\n",
        "The Target Encoder, denoted `f_θ'`, is structurally identical to the Backbone Decoder but its parameters `θ'` are an Exponential Moving Average (EMA) of the backbone parameters `θ`: `θ' ← α θ' + (1 - α) θ`. Crucially, it processes the input sequence non-causally.\n",
        "*   Input: Token sequence `X`.\n",
        "*   Processing: Identical transformer blocks as the backbone, but the self-attention mechanism is configured to be non-causal (bi-directional), allowing each position to attend to all other positions in the sequence (respecting padding).\n",
        "*   Output: A sequence of hidden states `H_{nc} = (h_{nc,1}, ..., h_{nc,T}) = f_θ'(X)`, where each `h_{nc,t}` depends on the entire sequence `x_1, ..., x_T`.\n",
        "*   Role: Generates the target representations for the JEPA prediction task. Its parameters are not updated via backpropagation; only through EMA updates from `f_θ`. These target representations reflect the model's accumulated \"memory\" or \"experience\" (`θ'`) applied to understand the full context of the current sequence `X`.\n",
        "\n",
        "Analogy: The Detective and the Full Reader: To build intuition for the causal/non-causal dynamic, consider the causal Backbone Decoder (`f_θ`) as a detective reading a novel one page at a time. When encountering missing pages (target spans), the detective can only guess their content based on what has been read so far (the causal context `H_c`). The non-causal Target Encoder (`f_θ'`) is like someone who has already read the entire book; they possess the complete understanding and \"memory\" of how those missing pages fit into the overall narrative. The output `H_{nc}` represents this perfect understanding, serving as the ground truth for what the detective should infer.\n",
        "\n",
        "3.3. Span Selection and Masking\n",
        "\n",
        "For the JEPA task, we sample multiple target spans within each sequence `X`.\n",
        "*   A proportion of the sequence is designated as the JEPA context.\n",
        "*   Several non-overlapping target spans `s_i = (start_i, end_i)` are randomly selected from the remaining portion.\n",
        "*   Parameters control the number of target spans, minimum/maximum span length, and context/target ratios.\n",
        "*   A `context_mask` indicates which positions belong to the context (1) and which belong to targets or padding (0).\n",
        "*   An `attention_mask` indicates padding tokens (0) vs. real tokens (1).\n",
        "\n",
        "3.4. Predictor\n",
        "\n",
        "The Predictor, `g_φ`, is another multi-layer Transformer-based module responsible for predicting the target representations.\n",
        "*   Input: A modified sequence where context positions retain their original embeddings (or potentially representations from `H_c`), while target positions are replaced with learnable `[MASK]` tokens/embeddings. Let this input be `X_masked`.\n",
        "*   Processing: The predictor consists of blocks performing:\n",
        "    1.  Causal Self-Attention: Operates on the `X_masked` sequence representation, using the `attention_mask` for padding but maintaining causality.\n",
        "    2.  Cross-Attention: Attends to the output states `H_c` from the causal Backbone Decoder. Queries come from the predictor's self-attention output. Keys and Values come from `H_c`. The `context_mask` is used here to ensure the predictor only attends to keys/values corresponding to the context positions in the backbone output.\n",
        "    3.  Feed-Forward Layers.\n",
        "*   Output: Predicted representations `P = (p_1, ..., p_T) = g_φ(X_masked, H_c, context_mask)`. We are interested in the outputs corresponding to the target spans: `P[s_i] = (p_{start_i}, ..., p_{end_i})`.\n",
        "*   Role: Learns to predict the non-causal target representations `H_{nc}[s_i]` using only the masked input and the causal context information from the backbone `H_c`. It acts like the detective's assistant, trying to reconstruct the meaning of the missing pages using only the clues gathered so far.\n",
        "\n",
        "3.5. Loss Functions\n",
        "\n",
        "The model is trained with a combined loss:\n",
        "`L_total = L_JEPA + λ * L_LM`\n",
        "\n",
        "*   JEPA Loss (`L_JEPA`): The Mean Squared Error (MSE) between the predictor's output embeddings and the target encoder's output embeddings for all target spans `s_i`:\n",
        "    `L_JEPA = (1 / N_spans) * Σ_{i} || P[s_i] - H_{nc}[s_i] ||^2`\n",
        "    (Normalization by span length or averaging per-span loss might also be considered). The target `H_{nc}[s_i]` is detached from the computation graph. This loss measures the difference between the assistant's guess and the full reader's knowledge.\n",
        "*   LM Loss (`L_LM`): The standard causal language modeling loss (Cross-Entropy) calculated using the output of the causal Backbone Decoder `H_c` and its associated LM head (which typically shares weights with the input embedding layer):\n",
        "    `L_LM = CrossEntropy(LM_Head(H_c), X_shifted)`\n",
        "*   `λ` is a hyperparameter balancing the two loss terms.\n",
        "\n",
        "3.6. Training\n",
        "\n",
        "Training proceeds by minimizing `L_total`. Parameters `θ` (backbone) and `φ` (predictor) are updated via gradient descent (e.g., AdamW [Loshchilov & Hutter, 2019]). Target encoder parameters `θ'` are updated using EMA after each optimizer step on `θ`.\n",
        "\n",
        "3.7. Mathematical Formulation\n",
        "\n",
        "To formalize the process during training and inference:\n",
        "\n",
        "Notation:\n",
        "*   `X = (x_1, ..., x_T)`: Input token sequence.\n",
        "*   `θ, θ', φ`: Parameters for backbone, target encoder (EMA of θ), and predictor.\n",
        "*   `f_θ`: Causal backbone decoder function.\n",
        "*   `f_θ'`: Non-causal target encoder function.\n",
        "*   `g_φ`: Predictor function.\n",
        "*   `LM_Head`: Language modeling head (linear layer).\n",
        "*   `H_c`: Hidden states from causal backbone. `H_c[t]` depends on `X_{1:t}`.\n",
        "*   `H_{nc}`: Hidden states from non-causal target encoder. `H_{nc}[t]` depends on `X_{1:T}`.\n",
        "*   `s = (start, end)`: Indices defining a target span.\n",
        "*   `X_masked`: Input to predictor with target spans masked.\n",
        "*   `Context(s)`: Indices of context tokens relevant for predicting span `s`.\n",
        "*   `P[s]`: Predicted representation for span `s`.\n",
        "*   `CE`: Cross-Entropy loss. `MSE`: Mean Squared Error loss. `detach`: Stop gradient.\n",
        "*   `α`: EMA decay rate. `λ`: LM loss weight. `∇`: Gradient. `LR`: Learning rate.\n",
        "*   `temp`: Temperature for sampling. `TopP`: Top-p sampling function.\n",
        "\n",
        "Training Step:\n",
        "1.  Causal Pass (Backbone): `H_c = f_θ(X)`\n",
        "2.  Non-Causal Pass (Target Encoder): `H_{nc} = f_θ'(X)` (with `torch.no_grad()`)\n",
        "3.  Prediction (Predictor): `P = g_φ(X_masked, H_c[Context(s)])` for all target spans `s`.\n",
        "4.  LM Loss: `L_LM = CE(LM_Head(H_c[:, :-1]), X[:, 1:])` (ignoring padding)\n",
        "5.  JEPA Loss: `L_JEPA = mean_s( MSE(P[s], detach(H_{nc}[s])) )`\n",
        "6.  Total Loss: `L_total = L_JEPA + λ * L_LM`\n",
        "7.  Parameter Update:\n",
        "    *   `θ ← θ - LR * ∇_θ L_total`\n",
        "    *   `φ ← φ - LR * ∇_φ L_total`\n",
        "    *   `θ' ← α * θ' + (1 - α) * θ`\n",
        "\n",
        "Inference (Autoregressive Generation):\n",
        "Given context `X_context = (x_1, ..., x_k)`:\n",
        "For `t` from `k` to `max_length - 1`:\n",
        "1.  Get Last Causal State: `h_{c,t} = f_θ(X_context)_{last_token_state}`\n",
        "2.  Get Logits: `logits_t = LM_Head(h_{c,t})`\n",
        "3.  Sample Next Token:\n",
        "    *   `probs_t = Softmax(TopP(logits_t / temp))`\n",
        "    *   `x_{t+1} ~ Multinomial(probs_t)`\n",
        "4.  Append: `X_context ← Append(X_context, x_{t+1})`\n",
        "Return `X_context`.\n",
        "*(Note: Only the causal backbone `f_θ` and `LM_Head` are used during inference)*.\n",
        "\n",
        "4. Experiments (Planned)\n",
        "\n",
        "*   Dataset: We plan to evaluate T-decoder-JEPA primarily on the GSM8K dataset [Cobbe et al., 2021], a collection of grade-school math word problems requiring multi-step reasoning.\n",
        "*   Baselines:\n",
        "    1.  A standard decoder-only model of identical size and configuration, trained solely with the causal LM objective (`λ=0`).\n",
        "    2.  (Optional) Other relevant SSL methods adapted to the decoder framework, if feasible.\n",
        "*   Evaluation Metrics:\n",
        "    1.  Accuracy on the GSM8K test set (final answer matching).\n",
        "    2.  Perplexity on a standard language modeling benchmark (e.g., WikiText) to assess the impact on general language modeling capabilities.\n",
        "    3.  JEPA prediction loss during training as a diagnostic metric.\n",
        "*   Ablation Studies: We will investigate the contribution of key components:\n",
        "    1.  Effect of the JEPA loss (varying `λ`).\n",
        "    2.  Importance of the non-causal target encoder (vs. using causal targets).\n",
        "    3.  Impact of the EMA update.\n",
        "    4.  Sensitivity to predictor depth and architecture.\n",
        "    5.  Influence of span selection hyperparameters.\n",
        "*   Hypothesized Results: We expect T-decoder-JEPA to achieve higher accuracy on GSM8K compared to the LM-only baseline, demonstrating improved reasoning capabilities stemming from the enriched representations learned via the JEPA objective. We anticipate that general LM performance (perplexity) will remain competitive or potentially improve slightly due to the regularizing effect of the JEPA task.\n",
        "\n",
        "(Table 1: Placeholder for GSM8K accuracy results for T-decoder-JEPA vs. baselines.)\n",
        "(Table 2: Placeholder for Perplexity results.)\n",
        "\n",
        "5. Discussion\n",
        "\n",
        "The core hypothesis behind T-decoder-JEPA is that forcing the model to predict future representations (derived non-causally) from a causal context compels it to learn more abstract and robust features. The non-causal target `H_{nc}` provides a rich signal about the complete sequence structure, which the predictor `g_φ` must learn to anticipate based only on the causally-generated context `H_c`. This implicit \"lookahead\" in the embedding space may encourage the backbone `f_θ` to encode information about long-range dependencies and underlying semantic/logical structure more effectively than relying solely on next-token prediction.\n",
        "\n",
        "Analogy: The Student and Teacher: The learning dynamic can be likened to training a student (the predictor `g_φ`) to predict the ending (target representation `H_{nc}[s]`) of a story by only reading the beginning (causal context representations `H_c`). The teacher (the non-causal target encoder `f_θ'`) knows the full story and provides the perfect target representation. The student makes a guess (`P[s]`), and the difference (JEPA loss) signals the error. This error signal not only improves the student (`g_φ`) but crucially puts pressure back on the causal decoder (`f_θ`)—the provider of the initial information—to generate richer, more predictive \"hints\" (the hidden states `H_c`) in the first place. Over time, the causal decoder learns to encode predictive signals about the future within its current state, even though it cannot see the future directly.\n",
        "\n",
        "Implications:\n",
        "*   Enhanced Representations: The JEPA task acts as a regularizer, potentially leading to representations less sensitive to superficial correlations and more attuned to deeper structure. The causal hidden states `H_c` become implicitly conditioned to support future prediction.\n",
        "*   Improved Reasoning & Downstream Tasks: By learning to anticipate representations related to future steps or outcomes, the model might develop capabilities more aligned with planning and multi-step reasoning. This should improve performance on downstream tasks like generation or classification that rely on these enriched causal states.\n",
        "\n",
        "Limitations:\n",
        "*   Computational Cost: The architecture involves forward passes through the backbone, target encoder, and predictor, increasing computational requirements compared to standard LM training.\n",
        "*   Hyperparameter Sensitivity: Performance might be sensitive to the choice of span selection strategy, the weighting factor `λ`, EMA decay `α`, and predictor architecture.\n",
        "*   Complexity: The interplay between the causal backbone, non-causal target, and predictor introduces complexity in analysis and debugging.\n",
        "\n",
        "Future Work:\n",
        "*   Scaling T-decoder-JEPA to larger models and datasets.\n",
        "*   Applying the architecture to other domains requiring long-range understanding or planning.\n",
        "*   In-depth analysis of the learned representations to understand the effects of the JEPA objective.\n",
        "*   Exploring alternative predictor designs or target generation strategies.\n",
        "\n",
        "6. Conclusion\n",
        "\n",
        "We introduced T-decoder-JEPA, a novel architecture that integrates the principles of Joint Embedding Predictive Architecture into a decoder-only transformer. By augmenting the standard causal language modeling objective with a task that predicts non-causal future representations from a causal context—using an EMA target encoder for ground truth and a dedicated predictor—T-decoder-JEPA aims to learn richer, more predictive internal representations. The training dynamic encourages the causal backbone to generate hidden states that are implicitly predictive of future sequence structure. We hypothesize this approach will enhance the capabilities of decoder-only models on complex reasoning tasks. Planned experiments on datasets like GSM8K will evaluate the effectiveness of this architecture compared to standard LM baselines. T-decoder-JEPA offers a promising direction for improving the representational power and reasoning abilities of large language models through principled self-supervised learning.\n",
        "\n",
        "7. References\n",
        "\n",
        "[Assran et al., 2023] Assran, M., et al. (2023). Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture. arXiv preprint arXiv:2301.08243.\n",
        "[Ba et al., 2016] Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016). Layer normalization. arXiv preprint arXiv:1607.06450.\n",
        "[Brown et al., 2020] Brown, T. B., et al. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33, 1877-1901.\n",
        "[Caruana, 1997] Caruana, R. (1997). Multitask learning. Machine learning, 28(1), 41-75.\n",
        "[Cobbe et al., 2021] Cobbe, K., et al. (2021). Training Verifiers to Solve Math Word Problems. arXiv preprint arXiv:2110.14168.\n",
        "[Devlin et al., 2019] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). Bert: Pre-training of deep bidirectional transformers for language understanding. Proceedings of NAACL-HLT 2019.\n",
        "[Gao et al., 2021] Gao, T., Yao, X., & Chen, D. (2021). SimCSE: Simple Contrastive Learning of Sentence Embeddings. arXiv preprint arXiv:2104.08821.\n",
        "[LeCun, 2022] LeCun, Y. (2022). A Path Towards Autonomous Machine Intelligence. OpenReview.\n",
        "[Logeswaran & Lee, 2018] Logeswaran, L., & Lee, H. (2018). An efficient framework for learning sentence representations. arXiv preprint arXiv:1803.02893.\n",
        "[Loshchilov & Hutter, 2019] Loshchilov, I., & Hutter, F. (2019). Decoupled weight decay regularization. ICLR 2019.\n",
        "[Mallen et al., 2023] Mallen, A., et al. (2023). When Do Pre-Training Objectives Help? An Empirical Study of Multi-Task Transfer Learning. arXiv preprint arXiv:2304.14748.\n",
        "[Nye et al., 2021] Nye, M., et al. (2021). Show Your Work: Scratchpads for Intermediate Computation with Language Models. arXiv preprint arXiv:2112.00114.\n",
        "[Radford et al., 2018] Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language understanding by generative pre-training. OpenAI Blog.\n",
        "[Radford et al., 2019] Radford, A., et al. (2019). Language models are unsupervised multitask learners. OpenAI Blog, 1(8), 9.\n",
        "[Raffel et al., 2020] Raffel, C., et al. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140), 1-67.\n",
        "[Su et al., 2024] Su, J., et al. (2024). Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568, 127063. (Note: Original RoPE work might be cited from earlier arXiv versions if preferred).\n",
        "[Touvron et al., 2023] Touvron, H., et al. (2023). Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.\n",
        "[Vaswani et al., 2017] Vaswani, A., et al. (2017). Attention is all you need. Advances in neural information processing systems, 30.\n",
        "[Vincent et al., 2008] Vincent, P., Larochelle, H., Bengio, Y., & Manzagol, P. A. (2008). Extracting and composing robust features with denoising autoencoders. ICML 2008.\n",
        "[Wei et al., 2022] Wei, J., et al. (2022). Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35, 24824-24837.\n",
        "[Xiong et al., 2020] Xiong, R., et al. (2020). On layer normalization in the transformer architecture. ICML 2020.\n"
      ],
      "metadata": {
        "id": "INSANn_qGJkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import copy\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "# ==========================================\n",
        "# 1) Hyperparameters\n",
        "# ==========================================\n",
        "def get_hyperparams():\n",
        "    return {\n",
        "        # Model Parameters\n",
        "        'batch_size': 2,\n",
        "        'block_size': 1024,               # INCREASED - More space for spans\n",
        "        'vocab_size': 256,\n",
        "        'embed_dim': 512,\n",
        "        'n_heads': 8,\n",
        "        'n_layers': 12,                    # Number of Decoder Blocks\n",
        "\n",
        "        # JEPA Parameters\n",
        "        'context_span_ratio': 0.6,        # Ratio calculation might need tuning with larger block size\n",
        "        'target_span_ratio': 0.2,         # Ratio calculation might need tuning with larger block size\n",
        "        'num_target_spans': 8,            # DECREASED - More realistic number\n",
        "        'min_span_length': 32,            # DECREASED - Easier to fit smaller spans\n",
        "\n",
        "        # Training Parameters\n",
        "        'num_epochs': 50,\n",
        "        'steps_per_epoch': 1000,\n",
        "        'eval_interval': 200,\n",
        "        'eval_iters': 100,\n",
        "        'ema_decay': 0.999,\n",
        "        'accumulation_steps': 8,\n",
        "        'lm_loss_weight': 0.92,\n",
        "\n",
        "        # Special Tokens\n",
        "        'bos_token': 254,\n",
        "        'eos_token': 255,\n",
        "        'pad_token': 0,\n",
        "\n",
        "        # Generation Parameters\n",
        "        'generate_num_tokens': 1024,      # Can match block_size or be different\n",
        "        'top_p': 0.8,\n",
        "        'start_prompt': \"Problem: A bakery produces cakes for $10 each. It costs them $5 in ingredients per cake, and they have a fixed overhead of $200 per day. How many cakes do they need to sell each day to make a daily profit of $100?\",\n",
        "\n",
        "        # Special Tags\n",
        "        'thinking_tag': \"<think>\",\n",
        "        'thinking_end_tag': \"</think>\",\n",
        "        'answer_tag': \"<answer>\",\n",
        "        'answer_end_tag': \"</answer>\",\n",
        "\n",
        "        # Paths & Modes\n",
        "        'checkpoint_path': \"t_jepa_mtl_decoder_rope_bs1024_checkpoint.pt\", # Updated name for new block size\n",
        "        'continue_training': True,\n",
        "        'system_prompt': \"\"\"Consider this math problem. Think step by step and provide your reasoning between <think> </think> tags, then give your final answer between <answer> </answer> tags.\"\"\"\n",
        "    }\n",
        "\n",
        "# ==========================================\n",
        "# 1.1) Select device\n",
        "# ==========================================\n",
        "def get_device():\n",
        "    device = \"mps\" if torch.backends.mps.is_available() else \\\n",
        "             (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    return device\n",
        "\n",
        "# ==========================================\n",
        "# 1.2) Data Loading and Preprocessing for GSM8K\n",
        "# ==========================================\n",
        "def load_gsm8k_data():\n",
        "    print(\"Loading GSM8K dataset...\")\n",
        "\n",
        "    try:\n",
        "        # Try using the 'datasets' library first\n",
        "        from datasets import load_dataset\n",
        "        # Specify cache_dir to avoid potential permission issues in default locations\n",
        "        cache_dir = os.path.expanduser(\"~/.cache/huggingface/datasets\")\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "        dataset = load_dataset(\"openai/gsm8k\", \"main\", cache_dir=cache_dir)\n",
        "        train_df = dataset[\"train\"].to_pandas()\n",
        "        test_df = dataset[\"test\"].to_pandas()\n",
        "        print(\"Dataset loaded using datasets library\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset with datasets library: {e}\")\n",
        "        print(\"Attempting alternative loading methods...\")\n",
        "        try:\n",
        "            # Alternative: Load directly from Hugging Face Hub parquet files\n",
        "            print(\"Attempting to load from Hugging Face Hub parquet files...\")\n",
        "            # Ensure you have pyarrow and fsspec installed: pip install pyarrow fsspec aiohttp\n",
        "            splits = {'train': 'main/train-00000-of-00001.parquet',\n",
        "                      'test': 'main/test-00000-of-00001.parquet'}\n",
        "            train_df = pd.read_parquet(\"hf://datasets/openai/gsm8k/\" + splits[\"train\"])\n",
        "            test_df = pd.read_parquet(\"hf://datasets/openai/gsm8k/\" + splits[\"test\"])\n",
        "            print(\"Dataset loaded using parquet files from Hugging Face Hub\")\n",
        "        except Exception as e2:\n",
        "            print(f\"Failed to load dataset using parquet from Hub: {e2}\")\n",
        "            # Fallback to local path if available (adjust path if needed)\n",
        "            local_path_train = \"./gsm8k_data/train.jsonl\" # Example local path\n",
        "            local_path_test = \"./gsm8k_data/test.jsonl\"   # Example local path\n",
        "            if os.path.exists(local_path_train) and os.path.exists(local_path_test):\n",
        "                 print(\"Attempting to load from local JSONL files...\")\n",
        "                 train_df = pd.read_json(local_path_train, lines=True)\n",
        "                 test_df = pd.read_json(local_path_test, lines=True)\n",
        "                 print(\"Dataset loaded from local JSONL files.\")\n",
        "            else:\n",
        "                print(f\"Local files not found at {local_path_train} and {local_path_test}\")\n",
        "                raise RuntimeError(\"Unable to load the GSM8K dataset via datasets, parquet, or local files.\")\n",
        "\n",
        "\n",
        "    print(f\"Training examples: {len(train_df)}\")\n",
        "    print(f\"Test examples: {len(test_df)}\")\n",
        "\n",
        "    # Split training data into train/validation\n",
        "    train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
        "\n",
        "    print(f\"Final training examples: {len(train_df)}\")\n",
        "    print(f\"Validation examples: {len(val_df)}\")\n",
        "    print(f\"Test examples: {len(test_df)}\")\n",
        "\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 1.3) Prepare data for JEPA training\n",
        "# ==========================================\n",
        "def prepare_batches_from_gsm8k(data_df, hyperparams, device):\n",
        "    \"\"\"Create training batches from GSM8K dataset with context and target spans for JEPA.\"\"\"\n",
        "    batch_indices = torch.randint(0, len(data_df), (hyperparams['batch_size'],))\n",
        "    batch_examples = data_df.iloc[batch_indices]\n",
        "\n",
        "    block_size = hyperparams['block_size']\n",
        "    bos_token = hyperparams['bos_token']\n",
        "    eos_token = hyperparams['eos_token']\n",
        "    pad_token = hyperparams['pad_token']\n",
        "\n",
        "    # JEPA specific parameters\n",
        "    num_target_spans = hyperparams['num_target_spans']\n",
        "    min_span_length = hyperparams['min_span_length']\n",
        "\n",
        "    # Create storage for batches\n",
        "    full_sequences = []\n",
        "    context_masks = [] # Mask for JEPA context (1=context, 0=target/padding)\n",
        "    target_spans_indices = [] # List of (start, end) tuples\n",
        "\n",
        "    for _, row in batch_examples.iterrows():\n",
        "        # Get question and answer\n",
        "        question = row['question']\n",
        "        answer = row['answer']\n",
        "\n",
        "        # Format with system prompt and tags\n",
        "        system_prompt = hyperparams['system_prompt']\n",
        "        full_text = f\"{system_prompt}\\n\\nProblem: {question}\\n\\n<think>{answer}</think>\\n\\n<answer>\"\n",
        "\n",
        "        # Extract the final answer from the explanation\n",
        "        answer_lines = answer.strip().split('\\n')\n",
        "        final_answer = answer_lines[-1] if answer_lines else \"\"\n",
        "        if \"answer is\" in final_answer.lower():\n",
        "            final_answer = final_answer.split(\"answer is\")[-1].strip()\n",
        "        elif \"=\" in final_answer:\n",
        "            final_answer = final_answer.split(\"=\")[-1].strip()\n",
        "        # Simple extraction, might need refinement\n",
        "        final_answer_numeric = ''.join(filter(lambda x: x.isdigit() or x == '.', final_answer.split('####')[-1].strip()))\n",
        "\n",
        "        full_text += f\"{final_answer_numeric}</answer>\"\n",
        "\n",
        "        # Convert to byte sequence and add BOS/EOS tokens\n",
        "        full_bytes = [bos_token] + [b for b in full_text.encode('utf-8', errors='replace')] + [eos_token]\n",
        "\n",
        "        # Truncate or pad sequence to block_size\n",
        "        seq_length = len(full_bytes)\n",
        "        if seq_length > block_size:\n",
        "            full_bytes = full_bytes[:block_size]\n",
        "            seq_length = block_size # Actual length after potential truncation\n",
        "        elif seq_length < block_size:\n",
        "            padding_needed = block_size - seq_length\n",
        "            full_bytes = full_bytes + [pad_token] * padding_needed\n",
        "            # seq_length remains the original length before padding\n",
        "\n",
        "        # Create context mask (1 = keep as context, 0 = mask for JEPA prediction)\n",
        "        # Initialize all non-padding positions as potential context\n",
        "        context_mask = torch.zeros(block_size, dtype=torch.float) # Use float for easier masking later\n",
        "        context_mask[:seq_length] = 1 # Only real tokens can be context initially\n",
        "\n",
        "        # Select random target spans for this example\n",
        "        current_target_spans = []\n",
        "        # Indices of real tokens available for masking\n",
        "        available_indices = torch.where(context_mask[:seq_length] == 1)[0].tolist()\n",
        "\n",
        "        for _ in range(num_target_spans):\n",
        "            # Check if enough *remaining* tokens are available to form a min_span_length span\n",
        "            if len(available_indices) < min_span_length:\n",
        "                break # Not enough remaining tokens\n",
        "\n",
        "            # Randomly choose target span length\n",
        "            # Max length: limited by available indices and a fraction of total real tokens\n",
        "            max_possible_len = min(len(available_indices), int(seq_length * 0.2)) # e.g., Max 20% of real tokens\n",
        "            if max_possible_len < min_span_length:\n",
        "                 continue # Skip if max possible length is too small\n",
        "\n",
        "            # Ensure span_length > min_span_length\n",
        "            span_length = torch.randint(min_span_length, max(min_span_length + 1, max_possible_len + 1), (1,)).item()\n",
        "\n",
        "            # Choose random starting position *from the list of available indices*\n",
        "            if len(available_indices) - span_length < 0:\n",
        "                # This shouldn't happen if max_possible_len logic is correct, but safety check\n",
        "                continue\n",
        "            start_idx_in_available = torch.randint(0, len(available_indices) - span_length + 1, (1,)).item()\n",
        "            start_pos = available_indices[start_idx_in_available]\n",
        "\n",
        "            # Calculate end position based on start_pos and span_length\n",
        "            # Ensure span doesn't exceed sequence length (should be covered by available_indices logic)\n",
        "            end_pos = min(start_pos + span_length, seq_length)\n",
        "            actual_span_length = end_pos - start_pos\n",
        "\n",
        "            # Skip very short spans that might result from hitting the seq_length boundary\n",
        "            if actual_span_length < min_span_length // 2:\n",
        "                continue\n",
        "\n",
        "            # Mark positions in target span on the context mask (set to 0)\n",
        "            context_mask[start_pos:end_pos] = 0\n",
        "\n",
        "            # Store span positions (start, end)\n",
        "            current_target_spans.append((start_pos, end_pos))\n",
        "\n",
        "            # Update available indices: remove indices used by the target span\n",
        "            new_available_indices = []\n",
        "            span_indices_set = set(range(start_pos, end_pos))\n",
        "            for idx in available_indices:\n",
        "                if idx not in span_indices_set:\n",
        "                    new_available_indices.append(idx)\n",
        "            available_indices = new_available_indices\n",
        "            # Check if we successfully removed indices\n",
        "            # print(f\" Span {start_pos}-{end_pos}, Remaining indices: {len(available_indices)}\") # Debug\n",
        "\n",
        "\n",
        "        # Add to batches\n",
        "        full_sequences.append(full_bytes)\n",
        "        context_masks.append(context_mask)\n",
        "        target_spans_indices.append(current_target_spans)\n",
        "        # if not current_target_spans: print(f\"Warning: No spans generated for an example. SeqLen: {seq_length}\") # Debug\n",
        "\n",
        "    # Convert to tensors\n",
        "    x = torch.tensor(full_sequences, dtype=torch.long).to(device)\n",
        "    context_masks = torch.stack(context_masks).to(device) # [B, T], 1 for context, 0 for target/padding\n",
        "\n",
        "    # Create attention mask (1 for real tokens including targets, 0 for padding)\n",
        "    # This mask is used by all attention layers to ignore padding\n",
        "    attention_mask = (x != pad_token).float().to(device) # [B, T], 1 for non-pad, 0 for pad\n",
        "\n",
        "    return x, context_masks, target_spans_indices, attention_mask\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 2) Rotary Positional Embedding (RoPE)\n",
        "# ==========================================\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim, max_seq_len=2048, base=10000, device=None):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        # Adjust max_seq_len for RoPE based on the actual block_size\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.base = base\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
        "\n",
        "        t = torch.arange(self.max_seq_len, device=device, dtype=self.inv_freq.dtype)\n",
        "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        self.register_buffer(\"cos_cached\", emb.cos(), persistent=False)\n",
        "        self.register_buffer(\"sin_cached\", emb.sin(), persistent=False)\n",
        "\n",
        "    def forward(self, seq_len: int):\n",
        "        # x: [*, seq_len, *]\n",
        "        # returns: cos, sin buffers of shape [seq_len, dim]\n",
        "        # Handle cases where seq_len might exceed precomputed length during generation potentially\n",
        "        if seq_len > self.max_seq_len:\n",
        "             # Dynamically extend RoPE if needed (more complex, often avoided by setting max_seq_len large enough)\n",
        "             # For now, we assume seq_len <= self.max_seq_len based on block_size\n",
        "             # Or simply clamp:\n",
        "             # print(f\"Warning: RoPE seq_len {seq_len} > max_seq_len {self.max_seq_len}. Clamping.\")\n",
        "             # seq_len = self.max_seq_len\n",
        "            raise ValueError(f\"RoPE sequence length {seq_len} exceeds precomputed max {self.max_seq_len}\")\n",
        "\n",
        "        return (\n",
        "            self.cos_cached[:seq_len, ...],\n",
        "            self.sin_cached[:seq_len, ...],\n",
        "        )\n",
        "\n",
        "def rotate_half(x):\n",
        "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
        "    x1 = x[..., : x.shape[-1] // 2]\n",
        "    x2 = x[..., x.shape[-1] // 2 :]\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "def apply_rotary_pos_emb(q, k, cos, sin):\n",
        "    \"\"\"Applies RoPE to query and key tensors.\"\"\"\n",
        "    # Add sequence length dimension if necessary\n",
        "    cos = cos.unsqueeze(0).unsqueeze(0) # [1, 1, T, D_head]\n",
        "    sin = sin.unsqueeze(0).unsqueeze(0) # [1, 1, T, D_head]\n",
        "\n",
        "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
        "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
        "    return q_embed, k_embed\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 3) Improved Attention Mechanism (with RoPE and Causal Masking)\n",
        "# ==========================================\n",
        "class ImprovedAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, n_heads, is_self_attention=True, use_rope=True, max_seq_len=2048):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = embed_dim // n_heads\n",
        "        assert self.head_dim * n_heads == self.embed_dim, \"embed_dim must be divisible by n_heads\"\n",
        "        self.is_self_attention = is_self_attention\n",
        "        self.use_rope = use_rope\n",
        "\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        # Instantiate RoPE only if used and needed\n",
        "        if self.use_rope and self.is_self_attention:\n",
        "            self.rotary_emb = RotaryEmbedding(self.head_dim, max_seq_len=max_seq_len)\n",
        "        else:\n",
        "            self.rotary_emb = None\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(0.1)\n",
        "        self.out_dropout = nn.Dropout(0.1)\n",
        "\n",
        "        # Buffer for causal mask (recreated if needed)\n",
        "        self.register_buffer(\"causal_mask_cache\", None, persistent=False)\n",
        "\n",
        "    def _get_causal_mask(self, T, device):\n",
        "        # Efficiently get or create causal mask\n",
        "        if self.causal_mask_cache is None or self.causal_mask_cache.shape[-1] < T:\n",
        "            # Create lower triangular mask (True for positions to be masked)\n",
        "            mask = torch.triu(torch.ones(T, T, dtype=torch.bool, device=device), diagonal=1)\n",
        "            self.causal_mask_cache = mask\n",
        "        # Return the sub-mask for the current sequence length T\n",
        "        # Ensure it's on the correct device (might change between train/eval/inference)\n",
        "        return self.causal_mask_cache[:T, :T].to(device=device)\n",
        "\n",
        "\n",
        "    def forward(self, x, attn_mask=None, key_value_states=None, is_causal=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Query input [B, T, C]\n",
        "            attn_mask: Padding mask [B, T_k] or broadcastable. 1=Keep, 0=Mask.\n",
        "            key_value_states: Optional key/value input for cross-attention [B, T_k, C].\n",
        "            is_causal: If True, apply causal masking (for self-attention only).\n",
        "        \"\"\"\n",
        "        B, T, C = x.size()\n",
        "        is_cross_attn = key_value_states is not None\n",
        "        # Determine if RoPE should be applied in this specific call\n",
        "        use_rope_for_this_pass = self.use_rope and self.is_self_attention and not is_cross_attn and self.rotary_emb is not None\n",
        "\n",
        "        # Project query\n",
        "        q = self.q_proj(x)\n",
        "\n",
        "        # Project keys and values\n",
        "        if is_cross_attn:\n",
        "            T_k = key_value_states.size(1)\n",
        "            k = self.k_proj(key_value_states)\n",
        "            v = self.v_proj(key_value_states)\n",
        "            # Causal mask is ignored in cross-attention\n",
        "            is_causal = False\n",
        "        else:\n",
        "            T_k = T # Self-attention\n",
        "            k = self.k_proj(x)\n",
        "            v = self.v_proj(x)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)    # B, H, T, D\n",
        "        k = k.view(B, T_k, self.n_heads, self.head_dim).transpose(1, 2)  # B, H, T_k, D\n",
        "        v = v.view(B, T_k, self.n_heads, self.head_dim).transpose(1, 2)  # B, H, T_k, D\n",
        "\n",
        "        # Apply RoPE if applicable\n",
        "        if use_rope_for_this_pass:\n",
        "            cos, sin = self.rotary_emb(T) # Get embeddings for query length T\n",
        "            q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
        "            scaling_factor = 1.0 # RoPE often doesn't need explicit scaling\n",
        "        else:\n",
        "            scaling_factor = 1.0 / math.sqrt(self.head_dim) # Standard scaling\n",
        "\n",
        "        # Compute scaled attention scores\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) * scaling_factor # [B, H, T, T_k]\n",
        "\n",
        "        # Apply combined masking (padding AND causal)\n",
        "        final_mask_bool = None # Boolean mask: True indicates position should be masked (-inf)\n",
        "\n",
        "        # 1. Process padding mask (attn_mask) -> masks Keys/Values\n",
        "        if attn_mask is not None:\n",
        "            # Input mask: 1=keep, 0=mask. We need True where mask should be applied.\n",
        "            if attn_mask.dim() == 2: # Common case: [B, T_k]\n",
        "                # Expand to broadcast: [B, T_k] -> [B, 1, 1, T_k]\n",
        "                padding_mask_bool = ~attn_mask.bool().unsqueeze(1).unsqueeze(2)\n",
        "            elif attn_mask.dim() == 4: # E.g., [B, 1, 1, T_k]\n",
        "                padding_mask_bool = ~attn_mask.bool()\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported attn_mask dimension: {attn_mask.dim()}\")\n",
        "            final_mask_bool = padding_mask_bool # [B, 1, 1, T_k]\n",
        "\n",
        "        # 2. Process causal mask (if self-attention and is_causal=True) -> masks future Query positions\n",
        "        if self.is_self_attention and is_causal:\n",
        "            causal_mask_bool = self._get_causal_mask(T, x.device) # [T, T]\n",
        "            # Expand to broadcast: [T, T] -> [1, 1, T, T]\n",
        "            causal_mask_bool = causal_mask_bool.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "            if final_mask_bool is not None:\n",
        "                # Combine: mask if *either* padding mask *or* causal mask applies\n",
        "                # Broadcasting works: [B, 1, 1, T_k] | [1, 1, T, T] -> [B, 1, T, T] (since T=T_k)\n",
        "                final_mask_bool = final_mask_bool | causal_mask_bool\n",
        "            else:\n",
        "                final_mask_bool = causal_mask_bool # [1, 1, T, T]\n",
        "\n",
        "        # Apply the combined mask to scores\n",
        "        if final_mask_bool is not None:\n",
        "             # Ensure mask shape is compatible with scores [B, H, T, T_k]\n",
        "             # final_mask_bool is typically [B, 1, T, T_k] or [B, 1, 1, T_k]\n",
        "             scores = scores.masked_fill(final_mask_bool, torch.finfo(scores.dtype).min)\n",
        "\n",
        "        # Apply softmax and dropout\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = self.attn_dropout(attn_weights)\n",
        "\n",
        "        # Apply attention to values\n",
        "        attn_output = torch.matmul(attn_weights, v) # [B, H, T, D]\n",
        "\n",
        "        # Reshape and project output\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.out_dropout(self.out_proj(attn_output))\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 4) Transformer Decoder Block\n",
        "# ==========================================\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, n_heads, dropout=0.1, max_seq_len=2048):\n",
        "        super().__init__()\n",
        "        # Using Pre-LN\n",
        "        self.ln1 = nn.LayerNorm(embed_dim)\n",
        "        self.self_attention = ImprovedAttention(embed_dim, n_heads, is_self_attention=True, use_rope=True, max_seq_len=max_seq_len)\n",
        "        self.ln2 = nn.LayerNorm(embed_dim)\n",
        "        hidden_dim = 4 * embed_dim\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.GELU(), # Consider SwiGLU later\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout) # Dropout for residual connections\n",
        "\n",
        "    def forward(self, x, attention_mask=None, is_causal=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input sequence [B, T, C].\n",
        "            attention_mask: Padding mask [B, T]. 1 for real tokens, 0 for padding.\n",
        "            is_causal: Whether the self-attention should be causal.\n",
        "        \"\"\"\n",
        "        # --- Self-Attention (Causal or Bidirectional based on is_causal) ---\n",
        "        residual = x\n",
        "        x_norm = self.ln1(x)\n",
        "        attn_output = self.self_attention(x_norm, attn_mask=attention_mask, is_causal=is_causal)\n",
        "        x = residual + self.dropout(attn_output)\n",
        "\n",
        "        # --- Feed-Forward ---\n",
        "        residual = x\n",
        "        x_norm = self.ln2(x)\n",
        "        ff_output = self.feed_forward(x_norm)\n",
        "        x = residual + self.dropout(ff_output)\n",
        "\n",
        "        return x\n",
        "\n",
        "# ==========================================\n",
        "# 5) JEPA Predictor Block (Causal Self-Attn, Cross-Attn to Decoder)\n",
        "# ==========================================\n",
        "class JEPAPredictorBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, n_heads, dropout=0.1, max_seq_len=2048):\n",
        "        super().__init__()\n",
        "        # Pre-LN structure\n",
        "        self.ln1 = nn.LayerNorm(embed_dim)\n",
        "        # Causal Self-attention within the predictor (RoPE enabled)\n",
        "        self.self_attention = ImprovedAttention(embed_dim, n_heads, is_self_attention=True, use_rope=True, max_seq_len=max_seq_len)\n",
        "\n",
        "        self.ln_cross_attn_query = nn.LayerNorm(embed_dim) # LN before cross-attn query input\n",
        "        self.ln_cross_attn_kv = nn.LayerNorm(embed_dim)    # LN before cross-attn key/value input\n",
        "        # Cross-attention to backbone decoder output (non-causal, no RoPE)\n",
        "        self.cross_attention = ImprovedAttention(embed_dim, n_heads, is_self_attention=False, use_rope=False, max_seq_len=max_seq_len)\n",
        "\n",
        "        self.ln3 = nn.LayerNorm(embed_dim)\n",
        "        hidden_dim = 4 * embed_dim\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, decoder_output, self_attention_mask=None, cross_attention_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Predictor input sequence [B, T, C].\n",
        "            decoder_output: Output from the main BackboneDecoder [B, T_kv, C].\n",
        "            self_attention_mask: Padding mask for predictor input [B, T]. 1=keep, 0=mask.\n",
        "            cross_attention_mask: Mask for decoder_output (keys/values in cross-attn) [B, T_kv].\n",
        "                                  Should be JEPA context_mask (1=context, 0=target/pad).\n",
        "        \"\"\"\n",
        "        # --- Causal Self-Attention within Predictor ---\n",
        "        residual = x\n",
        "        x_norm = self.ln1(x)\n",
        "        attn_output = self.self_attention(\n",
        "            x_norm,\n",
        "            attn_mask=self_attention_mask, # Use overall padding mask for self-attn\n",
        "            is_causal=True                 # Self-attention is CAUSAL\n",
        "        )\n",
        "        x = residual + self.dropout(attn_output)\n",
        "\n",
        "        # --- Cross-Attention to Decoder Output ---\n",
        "        residual = x\n",
        "        query_norm = self.ln_cross_attn_query(x)           # Normalize query input (from predictor state)\n",
        "        kv_norm = self.ln_cross_attn_kv(decoder_output)    # Normalize key/value input (from decoder)\n",
        "\n",
        "        cross_attn_output = self.cross_attention(\n",
        "            query_norm,                           # Query from predictor\n",
        "            attn_mask=cross_attention_mask,       # Mask K/V based on JEPA context_mask\n",
        "            key_value_states=kv_norm              # K/V from (normalized) decoder output\n",
        "        )\n",
        "        x = residual + self.dropout(cross_attn_output)\n",
        "\n",
        "        # --- Feed-Forward ---\n",
        "        residual = x\n",
        "        x_norm = self.ln3(x)\n",
        "        ff_output = self.feed_forward(x_norm)\n",
        "        x = residual + self.dropout(ff_output)\n",
        "\n",
        "        return x\n",
        "\n",
        "# ==========================================\n",
        "# 6) Backbone Decoder (Replaces ContextEncoder)\n",
        "# ==========================================\n",
        "class BackboneDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, n_heads, n_layers, block_size):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.dropout = nn.Dropout(0.1) # Dropout after embedding\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            DecoderBlock(embed_dim, n_heads, dropout=0.1, max_seq_len=block_size)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(embed_dim)\n",
        "        self.apply(self._init_weights) # Initialize weights\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None: torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias); torch.nn.init.ones_(module.weight)\n",
        "\n",
        "    def forward(self, x, attention_mask=None, is_causal=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input token indices [B, T].\n",
        "            attention_mask: Padding mask [B, T]. 1=keep, 0=mask.\n",
        "            is_causal: Controls self-attention masking in DecoderBlocks.\n",
        "        \"\"\"\n",
        "        B, T = x.size()\n",
        "        assert T <= self.block_size, f\"Sequence length {T} exceeds block size {self.block_size}\"\n",
        "\n",
        "        token_emb = self.token_embedding(x) # [B, T, C]\n",
        "        x = self.dropout(token_emb)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x, attention_mask=attention_mask, is_causal=is_causal)\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        return x\n",
        "\n",
        "# ==========================================\n",
        "# 7) JEPA Predictor (Using causal self-attn)\n",
        "# ==========================================\n",
        "class JEPAPredictor(nn.Module):\n",
        "    def __init__(self, embed_dim, n_heads, n_layers, block_size):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        # Consider using fewer layers for predictor, e.g., predictor_layers = n_layers // 2\n",
        "        predictor_layers = n_layers # Keep same depth for now\n",
        "\n",
        "        # Learnable mask token embedding\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        torch.nn.init.normal_(self.mask_token, mean=0.0, std=0.02)\n",
        "\n",
        "        # Predictor blocks (using JEPAPredictorBlock)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            JEPAPredictorBlock(embed_dim, n_heads, max_seq_len=block_size)\n",
        "            for _ in range(predictor_layers)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(embed_dim) # Final layer norm\n",
        "        self.apply(self._init_weights) # Initialize weights\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None: torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            pass # Only mask_token is an embedding here, initialized separately\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias); torch.nn.init.ones_(module.weight)\n",
        "\n",
        "    def forward(self, decoder_output_causal, target_spans_indices, context_mask, attention_mask):\n",
        "        \"\"\"\n",
        "        Predict target span representations.\n",
        "\n",
        "        Args:\n",
        "            decoder_output_causal: [B, T, C] Embeddings from CAUSAL BackboneDecoder pass.\n",
        "            target_spans_indices: List[List[Tuple[int, int]]] Target span indices.\n",
        "            context_mask: [B, T] JEPA context mask (1=context, 0=target/padding).\n",
        "            attention_mask: [B, T] Overall padding mask (1=real, 0=pad).\n",
        "\n",
        "        Returns:\n",
        "            List[List[Tensor]]: Predicted embeddings for target spans per batch item.\n",
        "        \"\"\"\n",
        "        B, T, C = decoder_output_causal.size()\n",
        "\n",
        "        # Initialize predictor input:\n",
        "        # Use mask tokens for target positions, causal decoder output for context positions.\n",
        "        predictor_input = torch.zeros_like(decoder_output_causal)\n",
        "        mask_token_expanded = self.mask_token.expand(B, T, C)\n",
        "\n",
        "        # Boolean masks for indexing\n",
        "        is_context = context_mask.bool()            # Where JEPA context mask is 1\n",
        "        is_target = (~is_context) & attention_mask.bool() # Where context is 0 AND not padding\n",
        "\n",
        "        # Populate predictor input\n",
        "        predictor_input[is_context] = decoder_output_causal[is_context]\n",
        "        predictor_input[is_target] = mask_token_expanded[is_target]\n",
        "        # Padding positions remain zero\n",
        "\n",
        "        # Process through predictor blocks\n",
        "        x = predictor_input\n",
        "        for block in self.blocks:\n",
        "            x = block(\n",
        "                x,\n",
        "                decoder_output=decoder_output_causal, # K/V for cross-attention comes from causal decoder\n",
        "                self_attention_mask=attention_mask,   # Padding mask for predictor's CAUSAL self-attention\n",
        "                cross_attention_mask=context_mask     # JEPA context mask to select K/V in cross-attention\n",
        "            )\n",
        "\n",
        "        x = self.ln_f(x) # Final normalization [B, T, C]\n",
        "\n",
        "        # Extract predicted embeddings only for the target spans\n",
        "        predicted_spans = []\n",
        "        for b in range(B):\n",
        "            batch_spans = []\n",
        "            if not target_spans_indices[b]: # Handle cases where no spans were generated for this item\n",
        "                predicted_spans.append(batch_spans)\n",
        "                continue\n",
        "            for start, end in target_spans_indices[b]:\n",
        "                valid_end = min(end, T) # Ensure end index is within bounds\n",
        "                if start < valid_end: # Ensure span has non-zero length\n",
        "                    span_emb = x[b, start:valid_end] # Extract embeddings [SpanLen, C]\n",
        "                    batch_spans.append(span_emb)\n",
        "            predicted_spans.append(batch_spans)\n",
        "\n",
        "        return predicted_spans\n",
        "\n",
        "# ==========================================\n",
        "# 8) Target Encoder (EMA copy of BackboneDecoder, runs NON-CAUSALLY)\n",
        "# ==========================================\n",
        "class TargetEncoder(nn.Module):\n",
        "    def __init__(self, backbone_decoder, ema_decay=0.999):\n",
        "        super().__init__()\n",
        "        # Create a deep copy of the backbone decoder structure\n",
        "        self.encoder = copy.deepcopy(backbone_decoder)\n",
        "        self.ema_decay = ema_decay\n",
        "        # Disable gradient computation for the target encoder\n",
        "        for param in self.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_ema(self, backbone_decoder, decay_rate=None):\n",
        "        \"\"\"Update target encoder weights using exponential moving average\"\"\"\n",
        "        decay_rate = decay_rate if decay_rate is not None else self.ema_decay\n",
        "        self.encoder.eval() # Ensure target is in eval mode\n",
        "        backbone_decoder.eval() # Ensure source is also in eval mode for consistency\n",
        "\n",
        "        source_params = dict(backbone_decoder.named_parameters())\n",
        "        target_params = dict(self.encoder.named_parameters())\n",
        "        assert source_params.keys() == target_params.keys(), \"Parameter mismatch between backbone and target encoders!\"\n",
        "\n",
        "        for name, source_param in source_params.items():\n",
        "            target_params[name].data.mul_(decay_rate).add_(source_param.data, alpha=1 - decay_rate)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        \"\"\"Forward pass for target encoder - runs NON-CAUSALLY\"\"\"\n",
        "        self.encoder.eval() # Ensure target encoder is always in eval mode\n",
        "        # Call the underlying decoder's forward pass, forcing is_causal=False\n",
        "        return self.encoder(x, attention_mask=attention_mask, is_causal=False)\n",
        "\n",
        "# ==========================================\n",
        "# 9) Complete T-JEPA Model (Decoder Backbone)\n",
        "# ==========================================\n",
        "class TJEPAModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, n_heads, n_layers, block_size, ema_decay=0.999, lm_loss_weight=0.1, pad_token_id=0):\n",
        "        super().__init__()\n",
        "        self.pad_token_id = pad_token_id\n",
        "        self.lm_loss_weight = lm_loss_weight\n",
        "        self.block_size = block_size # Store block_size\n",
        "\n",
        "        # Main Backbone: Transformer Decoder\n",
        "        self.decoder_backbone = BackboneDecoder(vocab_size, embed_dim, n_heads, n_layers, block_size)\n",
        "\n",
        "        # JEPA Predictor\n",
        "        self.predictor = JEPAPredictor(embed_dim, n_heads, n_layers, block_size)\n",
        "\n",
        "        # Target Encoder (EMA copy, runs non-causally)\n",
        "        self.target_encoder = TargetEncoder(self.decoder_backbone, ema_decay)\n",
        "        # Perform initial weight copy after TargetEncoder is created\n",
        "        self.target_encoder.update_ema(self.decoder_backbone, decay_rate=0.0)\n",
        "\n",
        "        # LM Head (predicts next token)\n",
        "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
        "\n",
        "        # Weight tying (tie backbone embedding with LM head)\n",
        "        self.decoder_backbone.token_embedding.weight = self.lm_head.weight\n",
        "\n",
        "    def forward(self, x, context_mask, target_spans_indices, attention_mask):\n",
        "        \"\"\"\n",
        "        Orchestrates the forward pass for training.\n",
        "\n",
        "        Args:\n",
        "            x: [B, T] Input token sequence.\n",
        "            context_mask: [B, T] JEPA context mask (1=context, 0=target/pad).\n",
        "            target_spans_indices: List[List[Tuple[int, int]]] Target span indices.\n",
        "            attention_mask: [B, T] Padding mask (1=real, 0=pad).\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing outputs needed for loss calculation.\n",
        "        \"\"\"\n",
        "\n",
        "        # 1. Causal pass through the main decoder backbone\n",
        "        # Used for LM loss and as context for the predictor's cross-attention.\n",
        "        decoder_output_causal = self.decoder_backbone(\n",
        "            x,\n",
        "            attention_mask=attention_mask,\n",
        "            is_causal=True # Standard causal operation\n",
        "        ) # [B, T, C]\n",
        "\n",
        "        # 2. Non-causal pass through the target encoder (EMA copy, no gradients)\n",
        "        # Used to get the target representations for the JEPA loss.\n",
        "        with torch.no_grad():\n",
        "            self.target_encoder.eval() # Ensure target is in eval mode\n",
        "            target_embeddings_full = self.target_encoder(\n",
        "                x,\n",
        "                attention_mask=attention_mask\n",
        "                # Internally calls backbone with is_causal=False\n",
        "            ) # [B, T, C]\n",
        "\n",
        "        # 3. Predictor pass\n",
        "        # Predicts representations for target spans using the causal decoder output\n",
        "        # as context in cross-attention.\n",
        "        predicted_spans_embeddings = self.predictor(\n",
        "            decoder_output_causal=decoder_output_causal, # Context for cross-attention\n",
        "            target_spans_indices=target_spans_indices,   # Which spans to predict\n",
        "            context_mask=context_mask,                   # Mask for cross-attention K/V\n",
        "            attention_mask=attention_mask                # Padding mask for predictor's self-attention\n",
        "        ) # List[List[Tensor]]\n",
        "\n",
        "        # 4. Extract actual target embeddings from the NON-CAUSAL target encoder output\n",
        "        target_spans_embeddings = []\n",
        "        for b in range(x.size(0)):\n",
        "            batch_spans = []\n",
        "            if not target_spans_indices[b]: # Handle empty span list for this batch item\n",
        "                target_spans_embeddings.append(batch_spans)\n",
        "                continue\n",
        "            for start, end in target_spans_indices[b]:\n",
        "                valid_end = min(end, x.size(1))\n",
        "                if start < valid_end:\n",
        "                    # Extract from the full target embeddings (non-causal)\n",
        "                    span_emb = target_embeddings_full[b, start:valid_end]\n",
        "                    batch_spans.append(span_emb)\n",
        "            target_spans_embeddings.append(batch_spans) # List[List[Tensor]]\n",
        "\n",
        "        # 5. Calculate LM Logits\n",
        "        # Based on the output of the CAUSAL decoder backbone pass.\n",
        "        lm_logits = self.lm_head(decoder_output_causal) # [B, T, VocabSize]\n",
        "\n",
        "        return {\n",
        "            \"predicted_spans_embeddings\": predicted_spans_embeddings, # From Predictor\n",
        "            \"target_spans_embeddings\": target_spans_embeddings,     # From Target Encoder (non-causal)\n",
        "            \"lm_logits\": lm_logits,                                 # From Backbone Decoder (causal)\n",
        "            \"input_sequence\": x,                                    # For LM loss calculation\n",
        "            \"attention_mask\": attention_mask,                       # For LM loss masking (optional)\n",
        "        }\n",
        "\n",
        "    def update_target_encoder(self):\n",
        "        \"\"\"Update target encoder weights using EMA\"\"\"\n",
        "        self.target_encoder.update_ema(self.decoder_backbone)\n",
        "\n",
        "    def compute_loss(self, outputs):\n",
        "        \"\"\"Compute combined JEPA (MSE) and LM (CrossEntropy) loss.\"\"\"\n",
        "        # --- JEPA MSE Loss ---\n",
        "        predicted_spans = outputs[\"predicted_spans_embeddings\"]\n",
        "        target_spans = outputs[\"target_spans_embeddings\"]\n",
        "        batch_size = len(predicted_spans)\n",
        "        jepa_losses = []\n",
        "        num_valid_comparisons = 0 # Track how many span comparisons actually happen\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            num_spans_in_batch_item = len(predicted_spans[b])\n",
        "            # Ensure target list has same length (should always be true if data prep is correct)\n",
        "            if num_spans_in_batch_item != len(target_spans[b]):\n",
        "                # print(f\"Warning: Mismatch in number of predicted ({num_spans_in_batch_item}) vs target ({len(target_spans[b])}) spans for batch item {b}.\")\n",
        "                continue # Skip this item if lengths mismatch\n",
        "\n",
        "            if num_spans_in_batch_item == 0:\n",
        "                continue # Skip if no spans were generated/extracted for this item\n",
        "\n",
        "            span_losses_for_batch_item = []\n",
        "            for i in range(num_spans_in_batch_item):\n",
        "                pred_span = predicted_spans[b][i] # [SpanLen_pred, C]\n",
        "                target_span = target_spans[b][i]  # [SpanLen_target, C]\n",
        "\n",
        "                # Ensure spans are not empty and shapes match exactly\n",
        "                if pred_span.nelement() > 0 and target_span.nelement() > 0 and pred_span.shape == target_span.shape:\n",
        "                    # Optional: Normalize embeddings before MSE loss\n",
        "                    # pred_span_norm = F.normalize(pred_span, p=2, dim=-1)\n",
        "                    # target_span_norm = F.normalize(target_span, p=2, dim=-1)\n",
        "                    # loss = F.mse_loss(pred_span_norm, target_span_norm)\n",
        "\n",
        "                    loss = F.mse_loss(pred_span, target_span)\n",
        "                    span_losses_for_batch_item.append(loss)\n",
        "                    num_valid_comparisons += 1\n",
        "                # else:\n",
        "                    # Optional: Log why a comparison was skipped\n",
        "                    # if pred_span.nelement() == 0: print(f\"Debug: Skipped empty pred span {b},{i}\")\n",
        "                    # elif target_span.nelement() == 0: print(f\"Debug: Skipped empty target span {b},{i}\")\n",
        "                    # else: print(f\"Debug: Skipped shape mismatch {pred_span.shape} vs {target_span.shape} for {b},{i}\")\n",
        "\n",
        "\n",
        "            # Average loss across valid spans for this batch item\n",
        "            if span_losses_for_batch_item:\n",
        "                 jepa_losses.append(torch.stack(span_losses_for_batch_item).mean())\n",
        "\n",
        "        # Average JEPA loss over the batch items that had valid spans\n",
        "        if jepa_losses:\n",
        "            final_jepa_loss = torch.stack(jepa_losses).mean()\n",
        "        else:\n",
        "            # Return zero loss if NO valid spans were compared across the entire batch\n",
        "            example_tensor = outputs[\"lm_logits\"] # Get device/dtype hint\n",
        "            final_jepa_loss = torch.tensor(0.0, device=example_tensor.device, dtype=example_tensor.dtype)\n",
        "            # if num_valid_comparisons == 0: print(\"Warning: JEPA loss is 0.0 because no valid span comparisons occurred in this batch.\")\n",
        "\n",
        "\n",
        "        # --- LM Cross Entropy Loss ---\n",
        "        lm_logits = outputs[\"lm_logits\"] # [B, T, V]\n",
        "        input_sequence = outputs[\"input_sequence\"] # [B, T]\n",
        "\n",
        "        # Shift logits and labels for next token prediction\n",
        "        shift_logits = lm_logits[:, :-1, :].contiguous() # [B, T-1, V]\n",
        "        shift_labels = input_sequence[:, 1:].contiguous() # [B, T-1]\n",
        "\n",
        "        # Flatten the tokens for CrossEntropyLoss\n",
        "        shift_logits = shift_logits.view(-1, shift_logits.size(-1)) # [B*(T-1), V]\n",
        "        shift_labels = shift_labels.view(-1) # [B*(T-1)]\n",
        "\n",
        "        # Calculate loss, ignoring padding tokens\n",
        "        lm_loss = F.cross_entropy(shift_logits, shift_labels, ignore_index=self.pad_token_id)\n",
        "\n",
        "        # --- Combine Losses ---\n",
        "        total_loss = final_jepa_loss + self.lm_loss_weight * lm_loss\n",
        "\n",
        "        return {\n",
        "            \"total_loss\": total_loss,\n",
        "            \"jepa_loss\": final_jepa_loss, # This should now be non-zero if spans are generated\n",
        "            \"lm_loss\": lm_loss\n",
        "        }\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, x, max_new_tokens, temperature=1.0, top_p=0.9):\n",
        "        \"\"\"Generate text autoregressively using the BackboneDecoder.\"\"\"\n",
        "        self.eval() # Ensure model is in evaluation mode\n",
        "        B = x.size(0)\n",
        "        pad_token_id = self.pad_token_id\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop context if it exceeds block size\n",
        "            x_cond = x if x.size(1) <= self.block_size else x[:, -self.block_size:]\n",
        "            seq_len = x_cond.size(1)\n",
        "\n",
        "            # Create attention mask for padding (1 for real tokens, 0 for padding)\n",
        "            attention_mask = (x_cond != pad_token_id).float() # [B, T]\n",
        "\n",
        "            # Get embeddings from the decoder backbone (CAUSALLY)\n",
        "            decoder_output = self.decoder_backbone(\n",
        "                x_cond,\n",
        "                attention_mask=attention_mask,\n",
        "                is_causal=True # Explicitly causal for generation\n",
        "            ) # [B, T, C]\n",
        "\n",
        "            # Get logits for the *next* token prediction (using the last token's embedding)\n",
        "            # Apply LM head to the embedding of the very last token in the sequence\n",
        "            logits = self.lm_head(decoder_output[:, -1, :])  # [B, C] -> [B, V]\n",
        "\n",
        "            # Apply temperature scaling\n",
        "            if temperature > 0 and temperature != 1.0:\n",
        "                 logits = logits / temperature\n",
        "\n",
        "            # Apply top-p (nucleus) sampling\n",
        "            if top_p > 0.0 and top_p < 1.0:\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                # Remove tokens with cumulative probability above the threshold\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                # Shift right to keep the first token above the threshold\n",
        "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                # Scatter mask back to original indices\n",
        "                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "                # Apply mask by setting logits to -infinity\n",
        "                logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "            # Sample from the potentially modified distribution\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1) # [B, 1]\n",
        "\n",
        "            # Append sampled token to the running sequence\n",
        "            x = torch.cat([x, next_token], dim=1)\n",
        "\n",
        "            # Optional: Check if EOS token was generated in *all* sequences (for batch generation)\n",
        "            # if hyperparams['eos_token'] is not None and (next_token == hyperparams['eos_token']).all():\n",
        "            #     break\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 10) Evaluation Function\n",
        "# ==========================================\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model, train_df, val_df, hyperparams, device):\n",
        "    \"\"\"Estimates loss on train and validation splits.\"\"\"\n",
        "    out = {}\n",
        "    model.eval() # Set model to evaluation mode\n",
        "\n",
        "    for split, df in [('train', train_df), ('val', val_df)]:\n",
        "        total_losses = torch.zeros(hyperparams['eval_iters'])\n",
        "        jepa_losses = torch.zeros(hyperparams['eval_iters'])\n",
        "        lm_losses = torch.zeros(hyperparams['eval_iters'])\n",
        "\n",
        "        # Use tqdm for eval iterations if desired, but can be removed\n",
        "        # pbar_eval = tqdm(range(hyperparams['eval_iters']), desc=f\"Eval {split}\", leave=False)\n",
        "        # for k in pbar_eval:\n",
        "        for k in range(hyperparams['eval_iters']):\n",
        "            # Get a batch of data\n",
        "            x, context_mask, target_spans_indices, attention_mask = prepare_batches_from_gsm8k(\n",
        "                df, hyperparams, device\n",
        "            )\n",
        "\n",
        "            # Forward pass through the model\n",
        "            outputs = model(x, context_mask, target_spans_indices, attention_mask)\n",
        "\n",
        "            # Compute loss using the model's loss function\n",
        "            loss_dict = model.compute_loss(outputs)\n",
        "\n",
        "            # Store losses\n",
        "            total_losses[k] = loss_dict['total_loss'].item()\n",
        "            jepa_losses[k] = loss_dict['jepa_loss'].item()\n",
        "            lm_losses[k] = loss_dict['lm_loss'].item()\n",
        "\n",
        "        # Calculate average losses for the split\n",
        "        out[split + '_total'] = total_losses.mean()\n",
        "        out[split + '_jepa'] = jepa_losses.mean()\n",
        "        out[split + '_lm'] = lm_losses.mean()\n",
        "\n",
        "    # model.train() # Caller should reset mode after evaluation\n",
        "    return out\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 11) Generate Text Function (Uses model.generate)\n",
        "# ==========================================\n",
        "@torch.no_grad()\n",
        "def generate_from_prompt(model, hyperparams, prompt_text=None, max_new_tokens=200, top_p=None, device=\"cuda\"):\n",
        "    \"\"\"Generates text from a prompt using the model's generate method.\"\"\"\n",
        "    model.eval() # Ensure evaluation mode\n",
        "    prompt_text = prompt_text if prompt_text is not None else hyperparams['start_prompt']\n",
        "    top_p = top_p if top_p is not None else hyperparams['top_p']\n",
        "    system_prompt = hyperparams['system_prompt']\n",
        "    full_prompt = f\"{system_prompt}\\n\\nProblem: {prompt_text}\\n\\n<think>\" # Start generation within think tags\n",
        "\n",
        "    # Encode prompt\n",
        "    bos_token = hyperparams['bos_token']\n",
        "    prompt_bytes = [bos_token] + [b for b in full_prompt.encode('utf-8', errors='replace')]\n",
        "    context = torch.tensor(prompt_bytes, dtype=torch.long, device=device).unsqueeze(0) # [1, T_prompt]\n",
        "\n",
        "    # Use the model's generate method\n",
        "    full_output_tokens = model.generate(\n",
        "        context,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        top_p=top_p,\n",
        "        temperature=0.8 # Example temperature, could be hyperparameter\n",
        "    ) # [1, T_prompt + T_new]\n",
        "\n",
        "    # Decode the full output sequence\n",
        "    full_output_list = full_output_tokens[0].tolist()\n",
        "\n",
        "    # Decode bytes to text, handling potential errors and special tokens\n",
        "    try:\n",
        "        # Find EOS token if present and truncate\n",
        "        eos_token = hyperparams['eos_token']\n",
        "        eos_pos = full_output_list.index(eos_token) if eos_token in full_output_list else -1\n",
        "        if eos_pos != -1:\n",
        "            full_output_list = full_output_list[:eos_pos] # Truncate at EOS\n",
        "\n",
        "        # Remove padding tokens and decode\n",
        "        pad_token = hyperparams['pad_token']\n",
        "        decoded_bytes = bytes([tok for tok in full_output_list if tok != pad_token])\n",
        "        generated_text = decoded_bytes.decode('utf-8', errors='replace')\n",
        "        return generated_text\n",
        "    except Exception as e:\n",
        "        print(f\"Decoding error during generation: {e}\")\n",
        "        # Fallback: return raw bytes representation\n",
        "        return str(bytes(full_output_list))\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 12) Token-by-Token Generation (Manual loop)\n",
        "# ==========================================\n",
        "@torch.no_grad()\n",
        "def generate_token_by_token(model, hyperparams, prompt_text, max_new_tokens=200, device=\"cuda\"):\n",
        "    \"\"\"Generates token by token, printing output, using the decoder model.\"\"\"\n",
        "    model.eval() # Ensure evaluation mode\n",
        "    system_prompt = hyperparams['system_prompt']\n",
        "    full_prompt = f\"{system_prompt}\\n\\nProblem: {prompt_text}\\n\\n<think>\"\n",
        "    bos_token = hyperparams['bos_token']\n",
        "    pad_token = hyperparams['pad_token']\n",
        "    eos_token = hyperparams['eos_token']\n",
        "\n",
        "    # Encode prompt\n",
        "    prompt_bytes = [bos_token] + [b for b in full_prompt.encode('utf-8', errors='replace')]\n",
        "    context = torch.tensor(prompt_bytes, dtype=torch.long, device=device).unsqueeze(0) # [1, T_prompt]\n",
        "\n",
        "    print(f\"\\n--- Generating from prompt ---\\n{full_prompt}\", end=\"\", flush=True)\n",
        "\n",
        "    generated_tokens = []\n",
        "    current_byte_fragment = b''\n",
        "\n",
        "    # Manually loop for token-by-token generation\n",
        "    for _ in range(max_new_tokens):\n",
        "        # --- Prepare input for this step ---\n",
        "        # Crop context if it exceeds block size\n",
        "        context_cond = context if context.size(1) <= model.block_size else context[:, -model.block_size:]\n",
        "        # Create attention mask for padding\n",
        "        attention_mask = (context_cond != pad_token).float() # [1, T_cond]\n",
        "\n",
        "        # --- Forward pass (Causal) ---\n",
        "        decoder_output = model.decoder_backbone(\n",
        "            context_cond,\n",
        "            attention_mask=attention_mask,\n",
        "            is_causal=True\n",
        "        ) # [1, T_cond, C]\n",
        "        # Get logits for the next token prediction (using the last token's output)\n",
        "        logits = model.lm_head(decoder_output[:, -1, :]) # [1, V]\n",
        "\n",
        "        # --- Sampling (Top-p) ---\n",
        "        top_p = hyperparams['top_p']\n",
        "        temperature = 0.8 # Example temperature\n",
        "        if temperature > 0 and temperature != 1.0:\n",
        "            logits = logits / temperature\n",
        "        if top_p > 0.0 and top_p < 1.0:\n",
        "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "            sorted_indices_to_remove = cumulative_probs > top_p\n",
        "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "            sorted_indices_to_remove[..., 0] = 0\n",
        "            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "            logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1) # [1, 1]\n",
        "\n",
        "        # --- Update context and decode/print ---\n",
        "        next_token_value = next_token.item()\n",
        "        context = torch.cat([context, next_token], dim=1) # Append to context for next step\n",
        "        generated_tokens.append(next_token_value)\n",
        "\n",
        "        # Attempt to decode and print the new byte(s)\n",
        "        current_byte_fragment += bytes([next_token_value])\n",
        "        try:\n",
        "            next_char = current_byte_fragment.decode('utf-8')\n",
        "            print(next_char, end=\"\", flush=True)\n",
        "            current_byte_fragment = b'' # Reset fragment if decode succeeds\n",
        "            time.sleep(0.01) # Small delay for visualization\n",
        "        except UnicodeDecodeError:\n",
        "            # If we can't decode (partial UTF-8 character), wait for more bytes\n",
        "            if len(current_byte_fragment) > 3: # Avoid getting stuck on invalid sequences\n",
        "                 print(\"<?>\", end=\"\", flush=True) # Print placeholder for invalid sequence\n",
        "                 current_byte_fragment = b'' # Reset\n",
        "\n",
        "        # Check for EOS token\n",
        "        if next_token_value == eos_token:\n",
        "            print(\"<EOS>\", end=\"\", flush=True)\n",
        "            break\n",
        "\n",
        "    print(\"\\n\\n--- Generation completed ---\")\n",
        "\n",
        "    # Return the full generated text (including prompt) after loop finishes\n",
        "    full_generated_list = prompt_bytes + generated_tokens\n",
        "    try:\n",
        "        eos_pos = full_generated_list.index(eos_token) if eos_token in full_generated_list else -1\n",
        "        if eos_pos != -1: full_generated_list = full_generated_list[:eos_pos]\n",
        "        decoded_bytes = bytes([tok for tok in full_generated_list if tok != pad_token])\n",
        "        return decoded_bytes.decode('utf-8', errors='replace')\n",
        "    except Exception as e:\n",
        "        print(f\"Final decoding error after token-by-token generation: {e}\")\n",
        "        return str(bytes(full_generated_list))\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 13) Training Implementation\n",
        "# ==========================================\n",
        "def train(continue_training=True):\n",
        "    \"\"\"Train the T-JEPA DECODER model on GSM8K.\"\"\"\n",
        "    # --- Setup ---\n",
        "    hyperparams = get_hyperparams()\n",
        "    device = get_device()\n",
        "    train_df, val_df, test_df = load_gsm8k_data()\n",
        "\n",
        "    # --- Model Initialization ---\n",
        "    model = TJEPAModel(\n",
        "        vocab_size=hyperparams['vocab_size'],\n",
        "        embed_dim=hyperparams['embed_dim'],\n",
        "        n_heads=hyperparams['n_heads'],\n",
        "        n_layers=hyperparams['n_layers'],\n",
        "        block_size=hyperparams['block_size'],\n",
        "        ema_decay=hyperparams['ema_decay'],\n",
        "        lm_loss_weight=hyperparams['lm_loss_weight'],\n",
        "        pad_token_id=hyperparams['pad_token']\n",
        "    ).to(device)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Model Block Size: {hyperparams['block_size']}\")\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\") # Includes decoder, predictor, LM head\n",
        "\n",
        "    # --- Optimizer ---\n",
        "    # Filter out parameters that don't require gradients (target encoder)\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        filter(lambda p: p.requires_grad, model.parameters()),\n",
        "        lr=3e-4, # Initial LR, scheduler will adjust\n",
        "        betas=(0.9, 0.95),\n",
        "        weight_decay=0.1\n",
        "    )\n",
        "\n",
        "    # --- Checkpoint Loading ---\n",
        "    start_epoch = 0\n",
        "    best_val_loss = float('inf')\n",
        "    current_step = 0\n",
        "    checkpoint_path = hyperparams['checkpoint_path']\n",
        "\n",
        "    if continue_training and os.path.exists(checkpoint_path):\n",
        "        print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
        "        try:\n",
        "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "            model_state = checkpoint['model_state']\n",
        "\n",
        "            # --- Flexible State Dict Loading ---\n",
        "            # Handles potential renames (e.g., encoder -> decoder) or minor architecture changes\n",
        "            current_model_dict = model.state_dict()\n",
        "            processed_state_dict = {}\n",
        "            warned_keys = set()\n",
        "            for k, v in model_state.items():\n",
        "                new_k = k\n",
        "                # Example rename: if checkpoint has 'context_encoder', map to 'decoder_backbone'\n",
        "                if k.startswith(\"context_encoder.\"):\n",
        "                    new_k = k.replace(\"context_encoder.\", \"decoder_backbone.\", 1)\n",
        "\n",
        "                if new_k in current_model_dict:\n",
        "                    if v.shape == current_model_dict[new_k].shape:\n",
        "                        processed_state_dict[new_k] = v\n",
        "                    else:\n",
        "                        if new_k not in warned_keys:\n",
        "                            print(f\"Warning: Shape mismatch for key '{new_k}'. Checkpoint: {v.shape}, Model: {current_model_dict[new_k].shape}. Skipping.\")\n",
        "                            warned_keys.add(new_k)\n",
        "                # else:\n",
        "                #     if k not in warned_keys and new_k not in warned_keys: # Avoid double warning if rename failed\n",
        "                #          print(f\"Warning: Key '{k}' (mapped to '{new_k}') not found in current model. Skipping.\")\n",
        "                #          warned_keys.add(k); warned_keys.add(new_k)\n",
        "\n",
        "            missing_keys, unexpected_keys = model.load_state_dict(processed_state_dict, strict=False)\n",
        "            if missing_keys: print(f\"Warning: Missing keys in final state_dict load: {missing_keys}\")\n",
        "            if unexpected_keys: print(f\"Warning: Unexpected keys in final state_dict load: {unexpected_keys}\")\n",
        "            # --- End Flexible State Dict Loading ---\n",
        "\n",
        "\n",
        "            # Load optimizer state cautiously\n",
        "            try:\n",
        "                # Basic check: does the number of parameter groups match?\n",
        "                if len(optimizer.param_groups) == len(checkpoint['optimizer_state']['param_groups']):\n",
        "                    # More thorough check: do parameter IDs seem to align? (Heuristic)\n",
        "                    # This is complex; often safer to reinitialize if model structure changed significantly.\n",
        "                    # For simplicity, we'll try loading if group count matches.\n",
        "                    optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
        "                    print(\"Optimizer state loaded.\")\n",
        "                else:\n",
        "                     print(\"Warning: Optimizer parameter group mismatch. Reinitializing optimizer.\")\n",
        "                     optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=3e-4, betas=(0.9, 0.95), weight_decay=0.1)\n",
        "            except Exception as e_optim:\n",
        "                 print(f\"Warning: Could not load optimizer state: {e_optim}. Reinitializing.\")\n",
        "                 optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=3e-4, betas=(0.9, 0.95), weight_decay=0.1)\n",
        "\n",
        "            # Load training progress\n",
        "            start_epoch = checkpoint.get('epoch', 0) + 1\n",
        "            best_val_loss = checkpoint.get('val_loss', float('inf'))\n",
        "            current_step = checkpoint.get('current_step', start_epoch * hyperparams['steps_per_epoch'])\n",
        "            print(f\"Resuming from epoch {start_epoch}, step {current_step}.\")\n",
        "\n",
        "            # IMPORTANT: Re-sync target encoder from the loaded backbone weights\n",
        "            model.target_encoder.update_ema(model.decoder_backbone, decay_rate=0.0)\n",
        "            print(\"Target encoder re-synced from loaded backbone weights.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading checkpoint comprehensively: {e}\")\n",
        "            print(\"Starting training from scratch or with partially loaded state.\")\n",
        "            start_epoch = 0; best_val_loss = float('inf'); current_step = 0\n",
        "            # Ensure target encoder is initialized correctly if loading failed\n",
        "            model.target_encoder.update_ema(model.decoder_backbone, decay_rate=0.0)\n",
        "\n",
        "    else:\n",
        "        print(\"Starting training from scratch.\")\n",
        "        # Initial sync of target encoder\n",
        "        model.target_encoder.update_ema(model.decoder_backbone, decay_rate=0.0)\n",
        "\n",
        "    # --- LR Scheduler ---\n",
        "    grad_clip = 1.0\n",
        "    total_steps = hyperparams['num_epochs'] * hyperparams['steps_per_epoch']\n",
        "    warmup_steps = 2000 # Example warmup steps\n",
        "    base_lr = 3e-4\n",
        "    min_lr = 1e-5\n",
        "\n",
        "    def get_lr(step):\n",
        "        # Cosine decay with warmup\n",
        "        if step < warmup_steps:\n",
        "            return base_lr * step / warmup_steps\n",
        "        decay_steps = total_steps - warmup_steps\n",
        "        steps_after_warmup = step - warmup_steps\n",
        "        if steps_after_warmup >= decay_steps: # Avoid going past total steps\n",
        "            return min_lr\n",
        "        cosine_decay = 0.5 * (1 + math.cos(math.pi * steps_after_warmup / decay_steps))\n",
        "        decayed_lr = min_lr + (base_lr - min_lr) * cosine_decay\n",
        "        return max(min_lr, decayed_lr) # Ensure LR doesn't drop below min_lr\n",
        "\n",
        "    # --- Training Loop ---\n",
        "    print(f\"Starting training on GSM8K dataset with T-JEPA (DECODER bs={hyperparams['block_size']} + RoPE + MTL)...\")\n",
        "    accumulation_steps = hyperparams['accumulation_steps']\n",
        "    # Optional: Setup Mixed Precision\n",
        "    # scaler = torch.cuda.amp.GradScaler(enabled=(device=='cuda'))\n",
        "\n",
        "    for epoch in range(start_epoch, hyperparams['num_epochs']):\n",
        "        print(f\"\\n--- Epoch {epoch+1}/{hyperparams['num_epochs']} ---\")\n",
        "        model.train() # Set model to training mode\n",
        "        epoch_total_loss, epoch_jepa_loss, epoch_lm_loss = 0.0, 0.0, 0.0\n",
        "        steps_in_epoch = hyperparams['steps_per_epoch']\n",
        "        optimizer.zero_grad() # Zero gradients at the start of epoch / after optimizer step\n",
        "\n",
        "        pbar = tqdm(range(steps_in_epoch), desc=f\"Epoch {epoch+1}\")\n",
        "        for step_in_epoch in pbar:\n",
        "            global_step = current_step\n",
        "\n",
        "            # --- Periodic Evaluation ---\n",
        "            if global_step > 0 and global_step % hyperparams['eval_interval'] == 0:\n",
        "                model.eval() # Switch to eval mode\n",
        "                losses = estimate_loss(model, train_df, val_df, hyperparams, device)\n",
        "                print(f\"\\nStep {global_step} Eval:\")\n",
        "                print(f\"  Train Total: {losses['train_total']:.4f}, JEPA: {losses['train_jepa']:.4f}, LM: {losses['train_lm']:.4f}\")\n",
        "                print(f\"  Val Total:   {losses['val_total']:.4f}, JEPA: {losses['val_jepa']:.4f}, LM: {losses['val_lm']:.4f}\")\n",
        "                model.train() # Switch back to train mode\n",
        "\n",
        "                # Save best model based on validation total loss\n",
        "                current_val_loss = losses['val_total']\n",
        "                if current_val_loss < best_val_loss:\n",
        "                    best_val_loss = current_val_loss\n",
        "                    save_path = checkpoint_path.replace('.pt', '_best.pt')\n",
        "                    # Save model state, optimizer, epoch, step, loss\n",
        "                    torch.save({\n",
        "                        'model_state': model.state_dict(),\n",
        "                        'optimizer_state': optimizer.state_dict(),\n",
        "                        'epoch': epoch,\n",
        "                        'current_step': global_step,\n",
        "                        'val_loss': best_val_loss,\n",
        "                        'hyperparams': hyperparams # Save hyperparams used for this checkpoint\n",
        "                    }, save_path)\n",
        "                    print(f\"  New best model saved to {save_path}! Val loss: {best_val_loss:.4f}\")\n",
        "\n",
        "            # --- Data Batch ---\n",
        "            try:\n",
        "                 x, context_mask, target_spans_indices, attention_mask = prepare_batches_from_gsm8k(\n",
        "                    train_df, hyperparams, device)\n",
        "            except Exception as data_err:\n",
        "                print(f\"\\nError preparing batch at step {global_step}: {data_err}. Skipping step.\")\n",
        "                # Skip optimizer step if accumulation would be incomplete\n",
        "                if (step_in_epoch + 1) % accumulation_steps == 0:\n",
        "                     optimizer.zero_grad() # Reset grads if skipping step at accumulation boundary\n",
        "                current_step += 1 # Still increment step counter\n",
        "                continue\n",
        "\n",
        "            # --- Forward and Loss Calculation ---\n",
        "            # Optional: Use autocast for mixed precision\n",
        "            # with torch.autocast(device_type=device if device != 'cpu' else 'cpu', dtype=torch.bfloat16 if device=='cuda' else torch.float32, enabled=(device=='cuda')):\n",
        "            outputs = model(x, context_mask, target_spans_indices, attention_mask)\n",
        "            loss_dict = model.compute_loss(outputs)\n",
        "            total_loss = loss_dict['total_loss']\n",
        "            jepa_loss = loss_dict['jepa_loss']\n",
        "            lm_loss = loss_dict['lm_loss']\n",
        "\n",
        "            # Scale loss for gradient accumulation\n",
        "            scaled_loss = total_loss / accumulation_steps\n",
        "\n",
        "            # --- Backward Pass ---\n",
        "            # scaler.scale(scaled_loss).backward() # With AMP\n",
        "            scaled_loss.backward() # Without AMP\n",
        "\n",
        "            # Accumulate epoch losses for monitoring (average per step)\n",
        "            epoch_total_loss += total_loss.item()\n",
        "            epoch_jepa_loss += jepa_loss.item()\n",
        "            epoch_lm_loss += lm_loss.item()\n",
        "\n",
        "            # --- Optimizer Step (after accumulation) ---\n",
        "            if (step_in_epoch + 1) % accumulation_steps == 0:\n",
        "                # Unscale gradients before clipping (required for AMP)\n",
        "                # scaler.unscale_(optimizer) # With AMP\n",
        "\n",
        "                # Clip gradients to prevent explosion\n",
        "                torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), grad_clip)\n",
        "\n",
        "                 # Check for NaN/Inf gradients *before* optimizer step\n",
        "                found_nan_inf = False\n",
        "                for p in filter(lambda p: p.requires_grad and p.grad is not None, model.parameters()):\n",
        "                    if not torch.isfinite(p.grad).all():\n",
        "                        print(f\"\\nWarning: NaN or Inf found in gradients at step {global_step}. Zeroing gradients for this step.\")\n",
        "                        found_nan_inf = True\n",
        "                        break\n",
        "                if found_nan_inf:\n",
        "                    optimizer.zero_grad() # Skip update if grads are invalid\n",
        "                else:\n",
        "                    # Update learning rate based on global step\n",
        "                    lr = get_lr(global_step)\n",
        "                    for param_group in optimizer.param_groups:\n",
        "                        param_group['lr'] = lr\n",
        "\n",
        "                    # Perform optimizer step\n",
        "                    # scaler.step(optimizer) # With AMP\n",
        "                    optimizer.step() # Without AMP\n",
        "\n",
        "                # Update target encoder using EMA after successful optimizer step\n",
        "                if not found_nan_inf:\n",
        "                     model.update_target_encoder()\n",
        "\n",
        "                # Update scaler for next iteration (AMP)\n",
        "                # scaler.update() # With AMP\n",
        "\n",
        "                # Zero gradients for the next accumulation cycle\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # --- Logging ---\n",
        "                # Calculate average loss over steps completed so far in the epoch\n",
        "                avg_total_loss = epoch_total_loss / (step_in_epoch + 1)\n",
        "                avg_jepa_loss = epoch_jepa_loss / (step_in_epoch + 1)\n",
        "                avg_lm_loss = epoch_lm_loss / (step_in_epoch + 1)\n",
        "                # Update tqdm progress bar\n",
        "                pbar.set_description(f\"E{epoch+1}, S{global_step+1}/{total_steps}, LR: {lr:.6f}\")\n",
        "                pbar.set_postfix({\n",
        "                    \"AvgLoss\": f\"{avg_total_loss:.4f}\",\n",
        "                    \"JEPA\": f\"{avg_jepa_loss:.4f}\", # Should be non-zero now\n",
        "                    \"LM\": f\"{avg_lm_loss:.4f}\",\n",
        "                    \"LastJEPA\": f\"{jepa_loss.item():.4f}\", # Show last step's JEPA loss\n",
        "                })\n",
        "\n",
        "            current_step += 1 # Increment global step counter\n",
        "\n",
        "        # --- End of Epoch ---\n",
        "        # Generate sample text\n",
        "        try:\n",
        "            print(\"\\nGenerating sample text at end of epoch...\")\n",
        "            model.eval() # Set to eval mode for generation\n",
        "            sample_text = generate_from_prompt(\n",
        "                model, hyperparams, hyperparams['start_prompt'],\n",
        "                max_new_tokens=256, # Shorter sample for epoch end\n",
        "                device=device\n",
        "            )\n",
        "            print(f\"Sample: {sample_text}\\n\" + \"-\"*20)\n",
        "            model.train() # Set back to train mode\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating sample: {e}\")\n",
        "            model.train() # Ensure model is back in train mode\n",
        "\n",
        "        # Save end-of-epoch checkpoint\n",
        "        torch.save({\n",
        "            'model_state': model.state_dict(),\n",
        "            'optimizer_state': optimizer.state_dict(),\n",
        "            'epoch': epoch,\n",
        "            'current_step': current_step,\n",
        "            'val_loss': best_val_loss, # Save the best validation loss seen so far\n",
        "            'hyperparams': hyperparams # Save hyperparams with checkpoint\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Checkpoint saved at end of epoch {epoch+1} to {checkpoint_path}.\")\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 14) Inference Implementation\n",
        "# ==========================================\n",
        "def inference(model_path, prompt_text, hyperparams_override=None):\n",
        "    \"\"\"Run inference with trained DECODER model.\"\"\"\n",
        "    device = get_device()\n",
        "\n",
        "    # --- Load Checkpoint and Hyperparameters ---\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Error: Model checkpoint not found at {model_path}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Loading model checkpoint from {model_path}...\")\n",
        "    try:\n",
        "        checkpoint = torch.load(model_path, map_location=device)\n",
        "        # Load hyperparams from checkpoint if available, otherwise use defaults/overrides\n",
        "        hyperparams_loaded = checkpoint.get('hyperparams', None)\n",
        "        if hyperparams_loaded:\n",
        "            print(\"Using hyperparameters loaded from checkpoint.\")\n",
        "            hyperparams = hyperparams_loaded\n",
        "        else:\n",
        "            print(\"Warning: Hyperparameters not found in checkpoint, using default values.\")\n",
        "            hyperparams = get_hyperparams()\n",
        "\n",
        "        # Allow overriding specific hyperparameters for inference\n",
        "        if hyperparams_override:\n",
        "            print(f\"Applying hyperparameter overrides: {hyperparams_override}\")\n",
        "            hyperparams.update(hyperparams_override)\n",
        "\n",
        "        print(f\"Using hyperparameters for inference: {hyperparams}\") # Log effective hyperparams\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading checkpoint structure: {e}\")\n",
        "        return None\n",
        "\n",
        "    # --- Create Model Structure based on loaded/effective Hyperparameters ---\n",
        "    try:\n",
        "        model = TJEPAModel(\n",
        "            vocab_size=hyperparams['vocab_size'], embed_dim=hyperparams['embed_dim'],\n",
        "            n_heads=hyperparams['n_heads'], n_layers=hyperparams['n_layers'],\n",
        "            block_size=hyperparams['block_size'], ema_decay=hyperparams['ema_decay'], # Needed for structure\n",
        "            lm_loss_weight=hyperparams['lm_loss_weight'], pad_token_id=hyperparams['pad_token']\n",
        "        ).to(device)\n",
        "    except KeyError as e:\n",
        "         print(f\"Error: Missing hyperparameter '{e}' needed to build the model structure.\")\n",
        "         return None\n",
        "\n",
        "    # --- Load Model State ---\n",
        "    try:\n",
        "        model_state = checkpoint['model_state']\n",
        "        # Flexible loading (handle potential renames/missing/unexpected keys)\n",
        "        current_model_dict = model.state_dict()\n",
        "        processed_state_dict = {}\n",
        "        for k, v in model_state.items():\n",
        "            new_k = k\n",
        "            if k.startswith(\"context_encoder.\"): new_k = k.replace(\"context_encoder.\", \"decoder_backbone.\", 1)\n",
        "            if new_k in current_model_dict and v.shape == current_model_dict[new_k].shape:\n",
        "                processed_state_dict[new_k] = v\n",
        "        missing, unexpected = model.load_state_dict(processed_state_dict, strict=False)\n",
        "        if missing: print(f\"  Info: Missing keys while loading state_dict: {missing}\")\n",
        "        if unexpected: print(f\"  Info: Unexpected keys while loading state_dict: {unexpected}\")\n",
        "        print(\"Model state loaded successfully.\")\n",
        "        loaded_epoch = checkpoint.get('epoch', -1); loaded_step = checkpoint.get('current_step', -1)\n",
        "        print(f\"  Checkpoint details: Epoch {loaded_epoch}, Step {loaded_step}, Val Loss {checkpoint.get('val_loss', 'N/A'):.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model state weights: {e}\")\n",
        "        print(\"Attempting inference with initialized model weights (may perform poorly).\")\n",
        "\n",
        "    # --- Run Generation ---\n",
        "    model.eval() # Set to evaluation mode\n",
        "    print(f\"\\n--- Generating response for prompt ---\")\n",
        "    print(f\"Prompt: {prompt_text}\")\n",
        "\n",
        "    # Use token-by-token generation for streaming output\n",
        "    result = generate_token_by_token(\n",
        "        model, hyperparams, prompt_text=prompt_text,\n",
        "        max_new_tokens=hyperparams.get('generate_num_tokens', 1024), # Use hyperparam, default 512\n",
        "        device=device\n",
        "    )\n",
        "    # Result is printed during generation\n",
        "\n",
        "    return result\n",
        "\n",
        "# ==========================================\n",
        "# 15) Main Entry Point\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Configuration ---\n",
        "    # Load default hyperparameters initially to get paths etc.\n",
        "    default_hyperparams = get_hyperparams()\n",
        "\n",
        "    # Choose mode: \"train\" or \"inference\"\n",
        "    MODE = \"train\"\n",
        "    # MODE = \"inference\"\n",
        "\n",
        "    # Set prompt for inference mode\n",
        "    INFERENCE_PROMPT = \"A rectangle has a length of 15 cm and a width of 8 cm. What is its perimeter and area?\"\n",
        "    # Specify model path for inference (usually the best saved model)\n",
        "    # Use path from default hyperparams, but it might be overridden if loaded from checkpoint in inference mode\n",
        "    INFERENCE_MODEL_PATH = default_hyperparams['checkpoint_path'].replace('.pt', '_best.pt')\n",
        "    # --- End Configuration ---\n",
        "\n",
        "\n",
        "    if MODE == \"train\":\n",
        "        print(\"Starting training...\")\n",
        "        # Pass continue_training flag from default hyperparams\n",
        "        train(continue_training=default_hyperparams['continue_training'])\n",
        "\n",
        "    elif MODE == \"inference\":\n",
        "        print(\"Starting inference...\")\n",
        "        # Check if the specified best model path exists, otherwise try the regular checkpoint\n",
        "        if not os.path.exists(INFERENCE_MODEL_PATH):\n",
        "             print(f\"Warning: Best model path '{INFERENCE_MODEL_PATH}' not found.\")\n",
        "             base_checkpoint_path = default_hyperparams['checkpoint_path']\n",
        "             if os.path.exists(base_checkpoint_path):\n",
        "                 print(f\"Attempting to use the base checkpoint path: '{base_checkpoint_path}'\")\n",
        "                 INFERENCE_MODEL_PATH = base_checkpoint_path\n",
        "             else:\n",
        "                 print(f\"Error: Neither best model nor base checkpoint path found ('{base_checkpoint_path}'). Cannot run inference.\")\n",
        "                 exit() # Exit if no model file found\n",
        "\n",
        "        # Run inference function\n",
        "        inference(INFERENCE_MODEL_PATH, INFERENCE_PROMPT) # Hyperparams will be loaded from checkpoint\n",
        "\n",
        "    else:\n",
        "        print(f\"Unknown mode: {MODE}. Choose 'train' or 'inference'.\")"
      ],
      "metadata": {
        "id": "VgVPI4sSSmoL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63883bf0-dec8-467d-ac02-d0d4a82efc73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Using device: cuda\n",
            "Loading GSM8K dataset...\n",
            "Error loading dataset with datasets library: No module named 'datasets'\n",
            "Attempting alternative loading methods...\n",
            "Attempting to load from Hugging Face Hub parquet files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded using parquet files from Hugging Face Hub\n",
            "Training examples: 7473\n",
            "Test examples: 1319\n",
            "Final training examples: 6725\n",
            "Validation examples: 748\n",
            "Test examples: 1319\n",
            "Model Block Size: 1024\n",
            "Total parameters: 126,309,888\n",
            "Trainable parameters: 88,367,616\n",
            "Loading checkpoint from t_jepa_mtl_decoder_rope_bs1024_checkpoint.pt...\n",
            "Optimizer state loaded.\n",
            "Resuming from epoch 41, step 41000.\n",
            "Target encoder re-synced from loaded backbone weights.\n",
            "Starting training on GSM8K dataset with T-JEPA (DECODER bs=1024 + RoPE + MTL)...\n",
            "\n",
            "--- Epoch 42/50 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 42:   0%|          | 0/1000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 41000 Eval:\n",
            "  Train Total: 1.0670, JEPA: 0.8443, LM: 0.2421\n",
            "  Val Total:   1.3609, JEPA: 0.8521, LM: 0.5531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E42, S41200/50000, LR: 0.000033:  20%|██        | 200/1000 [04:01<11:30,  1.16it/s, AvgLoss=1.0823, JEPA=0.8462, LM=0.2566, LastJEPA=0.9090]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 41200 Eval:\n",
            "  Train Total: 1.0567, JEPA: 0.8421, LM: 0.2333\n",
            "  Val Total:   1.3673, JEPA: 0.8525, LM: 0.5595\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E42, S41400/50000, LR: 0.000032:  40%|████      | 400/1000 [08:03<08:34,  1.17it/s, AvgLoss=1.0896, JEPA=0.8487, LM=0.2619, LastJEPA=0.8420]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 41400 Eval:\n",
            "  Train Total: 1.0660, JEPA: 0.8538, LM: 0.2306\n",
            "  Val Total:   1.3528, JEPA: 0.8424, LM: 0.5548\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E42, S41600/50000, LR: 0.000031:  60%|██████    | 600/1000 [12:05<05:46,  1.15it/s, AvgLoss=1.0829, JEPA=0.8462, LM=0.2572, LastJEPA=0.8483]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 41600 Eval:\n",
            "  Train Total: 1.0516, JEPA: 0.8496, LM: 0.2196\n",
            "  Val Total:   1.3555, JEPA: 0.8484, LM: 0.5512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E42, S41800/50000, LR: 0.000030:  80%|████████  | 800/1000 [16:06<02:51,  1.17it/s, AvgLoss=1.0811, JEPA=0.8454, LM=0.2562, LastJEPA=0.7833]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 41800 Eval:\n",
            "  Train Total: 1.0568, JEPA: 0.8455, LM: 0.2298\n",
            "  Val Total:   1.3415, JEPA: 0.8422, LM: 0.5427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E42, S42000/50000, LR: 0.000029: 100%|██████████| 1000/1000 [20:08<00:00,  1.21s/it, AvgLoss=1.0789, JEPA=0.8446, LM=0.2546, LastJEPA=0.7697]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating sample text at end of epoch...\n",
            "Sample: �Consider this math problem. Think step by step and provide your reasoning between <think> </think> tags, then give your final answer between <answer> </answer> tags.\n",
            "\n",
            "Problem: Problem: A bakery produces cakes for $10 each. It costs them $5 in ingredients per cake, and they have a fixed overhead of $200 per day. How many cakes do they need to sell each day to make a daily profit of $100?\n",
            "\n",
            "<think>The number of cakes that they produced is 10 * 10 = $<<10*10=100>>100.\n",
            "The fixed overhead and sell cake then to sell 100 - 20 = $<<100-20=80>>80.\n",
            "The number of cakes that they need to sell a day is 80 * $20 = $<<80*20=1600>>1600.\n",
            "#### 1600</think>\n",
            "\n",
            "<answer\n",
            "--------------------\n",
            "Checkpoint saved at end of epoch 42 to t_jepa_mtl_decoder_rope_bs1024_checkpoint.pt.\n",
            "\n",
            "--- Epoch 43/50 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 43:   0%|          | 0/1000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 42000 Eval:\n",
            "  Train Total: 1.0581, JEPA: 0.8399, LM: 0.2372\n",
            "  Val Total:   1.3533, JEPA: 0.8435, LM: 0.5541\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E43, S42200/50000, LR: 0.000028:  20%|██        | 200/1000 [04:02<11:34,  1.15it/s, AvgLoss=1.0731, JEPA=0.8428, LM=0.2504, LastJEPA=0.8019]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 42200 Eval:\n",
            "  Train Total: 1.0472, JEPA: 0.8383, LM: 0.2271\n",
            "  Val Total:   1.3633, JEPA: 0.8506, LM: 0.5572\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E43, S42400/50000, LR: 0.000028:  40%|████      | 400/1000 [08:04<08:33,  1.17it/s, AvgLoss=1.0775, JEPA=0.8465, LM=0.2511, LastJEPA=0.8113]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 42400 Eval:\n",
            "  Train Total: 1.0449, JEPA: 0.8430, LM: 0.2195\n",
            "  Val Total:   1.3786, JEPA: 0.8553, LM: 0.5688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E43, S42600/50000, LR: 0.000027:  60%|██████    | 600/1000 [12:06<05:44,  1.16it/s, AvgLoss=1.0777, JEPA=0.8479, LM=0.2498, LastJEPA=0.8763]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 42600 Eval:\n",
            "  Train Total: 1.0544, JEPA: 0.8490, LM: 0.2233\n",
            "  Val Total:   1.3480, JEPA: 0.8413, LM: 0.5507\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E43, S42800/50000, LR: 0.000026:  80%|████████  | 800/1000 [16:08<02:51,  1.16it/s, AvgLoss=1.0732, JEPA=0.8458, LM=0.2473, LastJEPA=0.7940]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 42800 Eval:\n",
            "  Train Total: 1.0561, JEPA: 0.8468, LM: 0.2275\n",
            "  Val Total:   1.3588, JEPA: 0.8426, LM: 0.5612\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E43, S43000/50000, LR: 0.000025: 100%|██████████| 1000/1000 [20:10<00:00,  1.21s/it, AvgLoss=1.0747, JEPA=0.8464, LM=0.2481, LastJEPA=0.8829]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating sample text at end of epoch...\n",
            "Sample: �Consider this math problem. Think step by step and provide your reasoning between <think> </think> tags, then give your final answer between <answer> </answer> tags.\n",
            "\n",
            "Problem: Problem: A bakery produces cakes for $10 each. It costs them $5 in ingredients per cake, and they have a fixed overhead of $200 per day. How many cakes do they need to sell each day to make a daily profit of $100?\n",
            "\n",
            "<think>The fixed overhead was $10 x 12 = $<<10*12=240>>240.\n",
            "Therefore, the fixed overhead was $240 x 5 = $<<240*5=1200>>1200.\n",
            "The fixed overhead was $50 x 1200 = $<<50*1200=60000>>60000.\n",
            "Therefore, the fixed overhead was $10000 - $60000 = $<<10000-60000=10000>>10\n",
            "--------------------\n",
            "Checkpoint saved at end of epoch 43 to t_jepa_mtl_decoder_rope_bs1024_checkpoint.pt.\n",
            "\n",
            "--- Epoch 44/50 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 44:   0%|          | 0/1000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 43000 Eval:\n",
            "  Train Total: 1.0539, JEPA: 0.8460, LM: 0.2259\n",
            "  Val Total:   1.3610, JEPA: 0.8474, LM: 0.5583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E44, S43200/50000, LR: 0.000024:  20%|██        | 200/1000 [04:02<11:23,  1.17it/s, AvgLoss=1.0725, JEPA=0.8494, LM=0.2424, LastJEPA=0.8188]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 43200 Eval:\n",
            "  Train Total: 1.0636, JEPA: 0.8464, LM: 0.2361\n",
            "  Val Total:   1.3980, JEPA: 0.8596, LM: 0.5853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E44, S43400/50000, LR: 0.000023:  40%|████      | 400/1000 [08:04<08:34,  1.17it/s, AvgLoss=1.0695, JEPA=0.8461, LM=0.2428, LastJEPA=0.7845]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 43400 Eval:\n",
            "  Train Total: 1.0617, JEPA: 0.8553, LM: 0.2243\n",
            "  Val Total:   1.3456, JEPA: 0.8415, LM: 0.5480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E44, S43600/50000, LR: 0.000023:  60%|██████    | 600/1000 [12:05<05:45,  1.16it/s, AvgLoss=1.0688, JEPA=0.8457, LM=0.2425, LastJEPA=0.8156]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 43600 Eval:\n",
            "  Train Total: 1.0525, JEPA: 0.8509, LM: 0.2191\n",
            "  Val Total:   1.3626, JEPA: 0.8487, LM: 0.5586\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E44, S43800/50000, LR: 0.000022:  80%|████████  | 800/1000 [16:07<02:51,  1.17it/s, AvgLoss=1.0682, JEPA=0.8458, LM=0.2418, LastJEPA=0.9056]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 43800 Eval:\n",
            "  Train Total: 1.0486, JEPA: 0.8467, LM: 0.2195\n",
            "  Val Total:   1.3486, JEPA: 0.8477, LM: 0.5445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E44, S44000/50000, LR: 0.000021: 100%|██████████| 1000/1000 [20:09<00:00,  1.21s/it, AvgLoss=1.0681, JEPA=0.8456, LM=0.2418, LastJEPA=0.9313]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating sample text at end of epoch...\n",
            "Sample: �Consider this math problem. Think step by step and provide your reasoning between <think> </think> tags, then give your final answer between <answer> </answer> tags.\n",
            "\n",
            "Problem: Problem: A bakery produces cakes for $10 each. It costs them $5 in ingredients per cake, and they have a fixed overhead of $200 per day. How many cakes do they need to sell each day to make a daily profit of $100?\n",
            "\n",
            "<think>The fixed ingredients produce is $10 x 12 = $<<10*12=120>>120.\n",
            "The fixed ingredients produce is $10 x 120 = $<<10*120=1200>>1200.\n",
            "Therefore, the fixed ingredients produced $1000 - $100= $<<1000-100=900>>900.\n",
            "#### 900</think>\n",
            "\n",
            "<answer>900</answer>\n",
            "--------------------\n",
            "Checkpoint saved at end of epoch 44 to t_jepa_mtl_decoder_rope_bs1024_checkpoint.pt.\n",
            "\n",
            "--- Epoch 45/50 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 45:   0%|          | 0/1000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 44000 Eval:\n",
            "  Train Total: 1.0630, JEPA: 0.8527, LM: 0.2286\n",
            "  Val Total:   1.3441, JEPA: 0.8424, LM: 0.5452\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E45, S44200/50000, LR: 0.000020:  20%|██        | 200/1000 [04:02<11:33,  1.15it/s, AvgLoss=1.0659, JEPA=0.8498, LM=0.2350, LastJEPA=0.8821]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 44200 Eval:\n",
            "  Train Total: 1.0485, JEPA: 0.8507, LM: 0.2150\n",
            "  Val Total:   1.3698, JEPA: 0.8446, LM: 0.5709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E45, S44400/50000, LR: 0.000020:  40%|████      | 400/1000 [08:04<08:36,  1.16it/s, AvgLoss=1.0668, JEPA=0.8512, LM=0.2343, LastJEPA=0.8749]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 44400 Eval:\n",
            "  Train Total: 1.0566, JEPA: 0.8555, LM: 0.2185\n",
            "  Val Total:   1.4002, JEPA: 0.8675, LM: 0.5790\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E45, S44600/50000, LR: 0.000019:  60%|██████    | 600/1000 [12:06<05:41,  1.17it/s, AvgLoss=1.0668, JEPA=0.8499, LM=0.2358, LastJEPA=0.8400]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 44600 Eval:\n",
            "  Train Total: 1.0381, JEPA: 0.8433, LM: 0.2118\n",
            "  Val Total:   1.3929, JEPA: 0.8561, LM: 0.5835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E45, S44800/50000, LR: 0.000018:  80%|████████  | 800/1000 [16:08<02:52,  1.16it/s, AvgLoss=1.0661, JEPA=0.8493, LM=0.2357, LastJEPA=0.8827]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 44800 Eval:\n",
            "  Train Total: 1.0545, JEPA: 0.8461, LM: 0.2265\n",
            "  Val Total:   1.3863, JEPA: 0.8485, LM: 0.5846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E45, S45000/50000, LR: 0.000018: 100%|██████████| 1000/1000 [20:10<00:00,  1.21s/it, AvgLoss=1.0655, JEPA=0.8488, LM=0.2355, LastJEPA=0.9246]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating sample text at end of epoch...\n",
            "Sample: �Consider this math problem. Think step by step and provide your reasoning between <think> </think> tags, then give your final answer between <answer> </answer> tags.\n",
            "\n",
            "Problem: Problem: A bakery produces cakes for $10 each. It costs them $5 in ingredients per cake, and they have a fixed overhead of $200 per day. How many cakes do they need to sell each day to make a daily profit of $100?\n",
            "\n",
            "<think>The fixed ingredients produced in ingredients for $10 each ingredient is 10*10 = $<<10*10=100>>100\n",
            "The fixed amount to the day profit in ingredients is $5*100 = $<<5*100=500>>500\n",
            "The new needs 100/200 = <<100/200=5>>5 days to sell the amount they need to s\n",
            "--------------------\n",
            "Checkpoint saved at end of epoch 45 to t_jepa_mtl_decoder_rope_bs1024_checkpoint.pt.\n",
            "\n",
            "--- Epoch 46/50 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 46:   0%|          | 0/1000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 45000 Eval:\n",
            "  Train Total: 1.0465, JEPA: 0.8427, LM: 0.2215\n",
            "  Val Total:   1.3687, JEPA: 0.8516, LM: 0.5620\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E46, S45200/50000, LR: 0.000017:  20%|██        | 200/1000 [04:02<11:30,  1.16it/s, AvgLoss=1.0702, JEPA=0.8531, LM=0.2360, LastJEPA=0.8117]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 45200 Eval:\n",
            "  Train Total: 1.0406, JEPA: 0.8463, LM: 0.2112\n",
            "  Val Total:   1.3861, JEPA: 0.8609, LM: 0.5710\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E46, S45400/50000, LR: 0.000017:  40%|████      | 400/1000 [08:03<08:34,  1.17it/s, AvgLoss=1.0657, JEPA=0.8523, LM=0.2320, LastJEPA=0.7619]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 45400 Eval:\n",
            "  Train Total: 1.0525, JEPA: 0.8465, LM: 0.2238\n",
            "  Val Total:   1.3824, JEPA: 0.8543, LM: 0.5740\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E46, S45600/50000, LR: 0.000016:  60%|██████    | 600/1000 [12:05<05:43,  1.17it/s, AvgLoss=1.0648, JEPA=0.8510, LM=0.2324, LastJEPA=0.8332]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 45600 Eval:\n",
            "  Train Total: 1.0295, JEPA: 0.8391, LM: 0.2069\n",
            "  Val Total:   1.3897, JEPA: 0.8470, LM: 0.5899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E46, S45800/50000, LR: 0.000015:  80%|████████  | 800/1000 [16:07<02:51,  1.16it/s, AvgLoss=1.0649, JEPA=0.8522, LM=0.2312, LastJEPA=0.8531]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 45800 Eval:\n",
            "  Train Total: 1.0461, JEPA: 0.8491, LM: 0.2142\n",
            "  Val Total:   1.3893, JEPA: 0.8475, LM: 0.5889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E46, S46000/50000, LR: 0.000015: 100%|██████████| 1000/1000 [20:09<00:00,  1.21s/it, AvgLoss=1.0639, JEPA=0.8517, LM=0.2307, LastJEPA=0.7758]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating sample text at end of epoch...\n",
            "Sample: �Consider this math problem. Think step by step and provide your reasoning between <think> </think> tags, then give your final answer between <answer> </answer> tags.\n",
            "\n",
            "Problem: Problem: A bakery produces cakes for $10 each. It costs them $5 in ingredients per cake, and they have a fixed overhead of $200 per day. How many cakes do they need to sell each day to make a daily profit of $100?\n",
            "\n",
            "<think>The fixed and produces $200 x 10/100 = $<<200*10/100=20>>20 a day.\n",
            "So, they need to sell $5 in ingredients per cake for $10 x 20 = $<<10*20=200>>200.\n",
            "Therefore, they need to sell a total of $200 + $200 = $<<200+200=400>>400.\n",
            "#### 400</think>\n",
            "\n",
            "<answer>400</\n",
            "--------------------\n",
            "Checkpoint saved at end of epoch 46 to t_jepa_mtl_decoder_rope_bs1024_checkpoint.pt.\n",
            "\n",
            "--- Epoch 47/50 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 47:   0%|          | 0/1000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 46000 Eval:\n",
            "  Train Total: 1.0476, JEPA: 0.8606, LM: 0.2033\n",
            "  Val Total:   1.3646, JEPA: 0.8525, LM: 0.5566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E47, S46200/50000, LR: 0.000014:  20%|██        | 200/1000 [04:02<11:29,  1.16it/s, AvgLoss=1.0589, JEPA=0.8521, LM=0.2247, LastJEPA=0.8357]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 46200 Eval:\n",
            "  Train Total: 1.0635, JEPA: 0.8644, LM: 0.2164\n",
            "  Val Total:   1.3697, JEPA: 0.8424, LM: 0.5732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E47, S46400/50000, LR: 0.000014:  40%|████      | 400/1000 [08:04<08:38,  1.16it/s, AvgLoss=1.0557, JEPA=0.8490, LM=0.2247, LastJEPA=0.9156]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 46400 Eval:\n",
            "  Train Total: 1.0433, JEPA: 0.8557, LM: 0.2040\n",
            "  Val Total:   1.3779, JEPA: 0.8518, LM: 0.5718\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E47, S46600/50000, LR: 0.000014:  60%|██████    | 600/1000 [12:05<05:42,  1.17it/s, AvgLoss=1.0563, JEPA=0.8503, LM=0.2239, LastJEPA=0.7871]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 46600 Eval:\n",
            "  Train Total: 1.0334, JEPA: 0.8474, LM: 0.2021\n",
            "  Val Total:   1.3780, JEPA: 0.8538, LM: 0.5698\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E47, S46800/50000, LR: 0.000013:  80%|████████  | 800/1000 [16:07<02:52,  1.16it/s, AvgLoss=1.0577, JEPA=0.8505, LM=0.2252, LastJEPA=0.8460]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 46800 Eval:\n",
            "  Train Total: 1.0417, JEPA: 0.8498, LM: 0.2086\n",
            "  Val Total:   1.3636, JEPA: 0.8388, LM: 0.5704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E47, S47000/50000, LR: 0.000013: 100%|██████████| 1000/1000 [20:09<00:00,  1.21s/it, AvgLoss=1.0578, JEPA=0.8506, LM=0.2252, LastJEPA=0.8780]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating sample text at end of epoch...\n",
            "Sample: �Consider this math problem. Think step by step and provide your reasoning between <think> </think> tags, then give your final answer between <answer> </answer> tags.\n",
            "\n",
            "Problem: Problem: A bakery produces cakes for $10 each. It costs them $5 in ingredients per cake, and they have a fixed overhead of $200 per day. How many cakes do they need to sell each day to make a daily profit of $100?\n",
            "\n",
            "<think>The number of cakes they have earned is $10 x 100 = $<<10*100=1000>>1000.\n",
            "To sell hear off cakes cost $1000 x 10/100 = $<<1000*10/100=100>>100.\n",
            "Therefore, the total number of cakes they need to sell it cost $1000 + $100 = $<<1000+100=1300.\n",
            "#### 1300</think\n",
            "--------------------\n",
            "Checkpoint saved at end of epoch 47 to t_jepa_mtl_decoder_rope_bs1024_checkpoint.pt.\n",
            "\n",
            "--- Epoch 48/50 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 48:   0%|          | 0/1000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 47000 Eval:\n",
            "  Train Total: 1.0312, JEPA: 0.8453, LM: 0.2021\n",
            "  Val Total:   1.4143, JEPA: 0.8684, LM: 0.5934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E48, S47200/50000, LR: 0.000012:  20%|██        | 200/1000 [04:02<11:33,  1.15it/s, AvgLoss=1.0635, JEPA=0.8504, LM=0.2317, LastJEPA=0.9261]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 47200 Eval:\n",
            "  Train Total: 1.0465, JEPA: 0.8570, LM: 0.2059\n",
            "  Val Total:   1.4166, JEPA: 0.8627, LM: 0.6021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E48, S47400/50000, LR: 0.000012:  40%|████      | 400/1000 [08:03<08:36,  1.16it/s, AvgLoss=1.0602, JEPA=0.8502, LM=0.2282, LastJEPA=0.8476]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 47400 Eval:\n",
            "  Train Total: 1.0448, JEPA: 0.8582, LM: 0.2028\n",
            "  Val Total:   1.3985, JEPA: 0.8589, LM: 0.5865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E48, S47600/50000, LR: 0.000012:  60%|██████    | 600/1000 [12:05<05:42,  1.17it/s, AvgLoss=1.0579, JEPA=0.8508, LM=0.2250, LastJEPA=0.8606]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 47600 Eval:\n",
            "  Train Total: 1.0471, JEPA: 0.8551, LM: 0.2086\n",
            "  Val Total:   1.3789, JEPA: 0.8510, LM: 0.5739\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E48, S47800/50000, LR: 0.000012:  80%|████████  | 800/1000 [16:07<02:51,  1.17it/s, AvgLoss=1.0577, JEPA=0.8514, LM=0.2243, LastJEPA=0.8613]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 47800 Eval:\n",
            "  Train Total: 1.0444, JEPA: 0.8566, LM: 0.2041\n",
            "  Val Total:   1.3934, JEPA: 0.8520, LM: 0.5885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E48, S48000/50000, LR: 0.000011: 100%|██████████| 1000/1000 [20:09<00:00,  1.21s/it, AvgLoss=1.0565, JEPA=0.8507, LM=0.2237, LastJEPA=0.9021]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating sample text at end of epoch...\n",
            "Sample: �Consider this math problem. Think step by step and provide your reasoning between <think> </think> tags, then give your final answer between <answer> </answer> tags.\n",
            "\n",
            "Problem: Problem: A bakery produces cakes for $10 each. It costs them $5 in ingredients per cake, and they have a fixed overhead of $200 per day. How many cakes do they need to sell each day to make a daily profit of $100?\n",
            "\n",
            "<think>The number of cakes that they have and they cost $200 because 100 / 5 = <<100/5=20>>20\n",
            "They need $5 left to sell because 5 x 10 = <<5*10=50>>50\n",
            "They need $200 because 200 / 50 = <<200/50=20>>20\n",
            "#### 20</think>\n",
            "\n",
            "<answer>20</answer>\n",
            "--------------------\n",
            "Checkpoint saved at end of epoch 48 to t_jepa_mtl_decoder_rope_bs1024_checkpoint.pt.\n",
            "\n",
            "--- Epoch 49/50 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 49:   0%|          | 0/1000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 48000 Eval:\n",
            "  Train Total: 1.0375, JEPA: 0.8544, LM: 0.1990\n",
            "  Val Total:   1.3531, JEPA: 0.8329, LM: 0.5655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E49, S48200/50000, LR: 0.000011:  20%|██        | 200/1000 [04:02<11:24,  1.17it/s, AvgLoss=1.0563, JEPA=0.8539, LM=0.2199, LastJEPA=0.7260]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 48200 Eval:\n",
            "  Train Total: 1.0373, JEPA: 0.8556, LM: 0.1975\n",
            "  Val Total:   1.3903, JEPA: 0.8517, LM: 0.5855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E49, S48400/50000, LR: 0.000011:  40%|████      | 400/1000 [08:04<08:35,  1.16it/s, AvgLoss=1.0552, JEPA=0.8511, LM=0.2219, LastJEPA=0.7851]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 48400 Eval:\n",
            "  Train Total: 1.0318, JEPA: 0.8528, LM: 0.1946\n",
            "  Val Total:   1.4218, JEPA: 0.8574, LM: 0.6135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E49, S48600/50000, LR: 0.000011:  60%|██████    | 600/1000 [12:06<05:47,  1.15it/s, AvgLoss=1.0565, JEPA=0.8521, LM=0.2222, LastJEPA=0.9391]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 48600 Eval:\n",
            "  Train Total: 1.0352, JEPA: 0.8559, LM: 0.1950\n",
            "  Val Total:   1.4152, JEPA: 0.8634, LM: 0.5998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E49, S48800/50000, LR: 0.000010:  80%|████████  | 800/1000 [16:07<02:51,  1.17it/s, AvgLoss=1.0553, JEPA=0.8532, LM=0.2196, LastJEPA=0.8347]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 48800 Eval:\n",
            "  Train Total: 1.0403, JEPA: 0.8541, LM: 0.2024\n",
            "  Val Total:   1.4002, JEPA: 0.8452, LM: 0.6033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E49, S49000/50000, LR: 0.000010: 100%|██████████| 1000/1000 [20:09<00:00,  1.21s/it, AvgLoss=1.0555, JEPA=0.8536, LM=0.2195, LastJEPA=0.8818]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating sample text at end of epoch...\n",
            "Sample: �Consider this math problem. Think step by step and provide your reasoning between <think> </think> tags, then give your final answer between <answer> </answer> tags.\n",
            "\n",
            "Problem: Problem: A bakery produces cakes for $10 each. It costs them $5 in ingredients per cake, and they have a fixed overhead of $200 per day. How many cakes do they need to sell each day to make a daily profit of $100?\n",
            "\n",
            "<think>The number of cakes that the fixed overhead was $10/3 = $<<10/3=0.50>>0.50.\n",
            "To sell the overhead and cost $200 for a total of $10 for a total of $0.50 + $0.50 for a certain cost $0.50 + $0.50 for a total of $100 + 0.50 = $<<100+0.50=2.00>>2.00.\n",
            "Therefore, \n",
            "--------------------\n",
            "Checkpoint saved at end of epoch 49 to t_jepa_mtl_decoder_rope_bs1024_checkpoint.pt.\n",
            "\n",
            "--- Epoch 50/50 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 50:   0%|          | 0/1000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 49000 Eval:\n",
            "  Train Total: 1.0404, JEPA: 0.8565, LM: 0.1999\n",
            "  Val Total:   1.3893, JEPA: 0.8496, LM: 0.5866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E50, S49200/50000, LR: 0.000010:  20%|██        | 200/1000 [04:02<11:24,  1.17it/s, AvgLoss=1.0513, JEPA=0.8533, LM=0.2153, LastJEPA=0.6665]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 49200 Eval:\n",
            "  Train Total: 1.0227, JEPA: 0.8475, LM: 0.1905\n",
            "  Val Total:   1.3964, JEPA: 0.8557, LM: 0.5877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E50, S49400/50000, LR: 0.000010:  40%|████      | 400/1000 [08:03<08:34,  1.17it/s, AvgLoss=1.0481, JEPA=0.8514, LM=0.2137, LastJEPA=0.7557]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 49400 Eval:\n",
            "  Train Total: 1.0399, JEPA: 0.8573, LM: 0.1985\n",
            "  Val Total:   1.3951, JEPA: 0.8590, LM: 0.5827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E50, S49600/50000, LR: 0.000010:  60%|██████    | 600/1000 [12:05<05:46,  1.15it/s, AvgLoss=1.0513, JEPA=0.8541, LM=0.2144, LastJEPA=0.7783]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 49600 Eval:\n",
            "  Train Total: 1.0471, JEPA: 0.8590, LM: 0.2045\n",
            "  Val Total:   1.3947, JEPA: 0.8572, LM: 0.5843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E50, S49800/50000, LR: 0.000010:  80%|████████  | 800/1000 [16:07<02:51,  1.17it/s, AvgLoss=1.0518, JEPA=0.8540, LM=0.2150, LastJEPA=0.9536]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 49800 Eval:\n",
            "  Train Total: 1.0269, JEPA: 0.8455, LM: 0.1971\n",
            "  Val Total:   1.4198, JEPA: 0.8656, LM: 0.6025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E50, S50000/50000, LR: 0.000010: 100%|██████████| 1000/1000 [20:08<00:00,  1.21s/it, AvgLoss=1.0524, JEPA=0.8538, LM=0.2158, LastJEPA=0.7730]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating sample text at end of epoch...\n",
            "Sample: �Consider this math problem. Think step by step and provide your reasoning between <think> </think> tags, then give your final answer between <answer> </answer> tags.\n",
            "\n",
            "Problem: Problem: A bakery produces cakes for $10 each. It costs them $5 in ingredients per cake, and they have a fixed overhead of $200 per day. How many cakes do they need to sell each day to make a daily profit of $100?\n",
            "\n",
            "<think>The fixed overhead will need $5 * 10 = $<<5*10=50>>50 per day.\n",
            "The fixed overhead will need 50 * 10 = $<<50*10=500>>500 for the cakes.\n",
            "Therefore, they need to sell a fixed overhead will need 500 / 20 = <<500/20=25>>25 cakes.\n",
            "#### 25</think>\n",
            "\n",
            "<answer>25</an\n",
            "--------------------\n",
            "Checkpoint saved at end of epoch 50 to t_jepa_mtl_decoder_rope_bs1024_checkpoint.pt.\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import copy\n",
        "import os\n",
        "import time\n",
        "from collections import Counter\n",
        "from typing import Optional, Tuple, List, Dict, Any\n",
        "\n",
        "# ==========================================\n",
        "# Configuration\n",
        "# ==========================================\n",
        "MODEL_PATH = \"t_jepa_mtl_decoder_rope_bs1024_checkpoint.pt\" # Or your specific checkpoint path\n",
        "PROMPT_TEXT = \"what is five plus two?\"\n",
        "NUM_VOTES = 8 # K: Number of samples per token\n",
        "MAX_NEW_TOKENS = 2048 # L: Maximum generation length\n",
        "TEMPERATURE = 0.7 # Sampling temperature\n",
        "TOP_P = 0.9 # Top-p nucleus sampling\n",
        "SYSTEM_PROMPT = \"\"\"Consider this math problem. Think step by step and provide your reasoning between <think> </think> tags, then give your final answer between <answer> </answer> tags.\"\"\"\n",
        "THINK_TAG_START = \"<think>\" # Used to start generation after the prompt\n",
        "\n",
        "# ==========================================\n",
        "# Model Definitions (Copied from training script)\n",
        "# ==========================================\n",
        "\n",
        "# --- 1) Hyperparameters Default ---\n",
        "# These will be OVERRIDDEN by checkpoint if available\n",
        "def get_default_hyperparams():\n",
        "    # Provide sensible defaults in case checkpoint doesn't contain hyperparams\n",
        "    return {\n",
        "        'vocab_size': 256, 'embed_dim': 512, 'n_heads': 8, 'n_layers': 12,\n",
        "        'block_size': 1024, 'ema_decay': 0.999, 'lm_loss_weight': 0.1,\n",
        "        'bos_token': 254, 'eos_token': 255, 'pad_token': 0,\n",
        "        'top_p': 0.8, # Default for generation if not overridden\n",
        "        # JEPA params (not strictly needed for inference, but part of model structure)\n",
        "        'context_span_ratio': 0.6, 'target_span_ratio': 0.2,\n",
        "        'num_target_spans': 8, 'min_span_length': 32,\n",
        "        # Tags (used in helper functions)\n",
        "        'thinking_tag': \"<think>\", 'thinking_end_tag': \"</think>\",\n",
        "        'answer_tag': \"<answer>\", 'answer_end_tag': \"</answer>\",\n",
        "        'system_prompt': \"\"\"Consider this math problem. Think step by step and provide your reasoning between <think> </think> tags, then give your final answer between <answer> </answer> tags.\"\"\"\n",
        "    }\n",
        "\n",
        "# --- 2) RoPE ---\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim, max_seq_len=2048, base=10000, device=None):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.base = base\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
        "        t = torch.arange(self.max_seq_len, device=device, dtype=self.inv_freq.dtype)\n",
        "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        self.register_buffer(\"cos_cached\", emb.cos(), persistent=False)\n",
        "        self.register_buffer(\"sin_cached\", emb.sin(), persistent=False)\n",
        "\n",
        "    def forward(self, seq_len: int):\n",
        "        if seq_len > self.max_seq_len:\n",
        "             print(f\"Warning: RoPE seq_len {seq_len} > max_seq_len {self.max_seq_len}. Clamping.\")\n",
        "             seq_len = self.max_seq_len\n",
        "            # raise ValueError(f\"RoPE sequence length {seq_len} exceeds precomputed max {self.max_seq_len}\") # Clamp instead\n",
        "\n",
        "        return (\n",
        "            self.cos_cached[:seq_len, ...],\n",
        "            self.sin_cached[:seq_len, ...],\n",
        "        )\n",
        "\n",
        "def rotate_half(x):\n",
        "    x1 = x[..., : x.shape[-1] // 2]\n",
        "    x2 = x[..., x.shape[-1] // 2 :]\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "def apply_rotary_pos_emb(q, k, cos, sin):\n",
        "    cos = cos.unsqueeze(0).unsqueeze(0)\n",
        "    sin = sin.unsqueeze(0).unsqueeze(0)\n",
        "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
        "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
        "    return q_embed, k_embed\n",
        "\n",
        "# --- 3) Attention ---\n",
        "class ImprovedAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, n_heads, is_self_attention=True, use_rope=True, max_seq_len=2048):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = embed_dim // n_heads\n",
        "        assert self.head_dim * n_heads == self.embed_dim, \"embed_dim must be divisible by n_heads\"\n",
        "        self.is_self_attention = is_self_attention\n",
        "        self.use_rope = use_rope\n",
        "\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        if self.use_rope and self.is_self_attention:\n",
        "            self.rotary_emb = RotaryEmbedding(self.head_dim, max_seq_len=max_seq_len)\n",
        "        else:\n",
        "            self.rotary_emb = None\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(0.1)\n",
        "        self.out_dropout = nn.Dropout(0.1)\n",
        "        self.register_buffer(\"causal_mask_cache\", None, persistent=False)\n",
        "\n",
        "    def _get_causal_mask(self, T, device):\n",
        "        if self.causal_mask_cache is None or self.causal_mask_cache.shape[-1] < T:\n",
        "            mask = torch.triu(torch.ones(T, T, dtype=torch.bool, device=device), diagonal=1)\n",
        "            self.causal_mask_cache = mask\n",
        "        return self.causal_mask_cache[:T, :T].to(device=device)\n",
        "\n",
        "    def forward(self, x, attn_mask=None, key_value_states=None, is_causal=False):\n",
        "        B, T, C = x.size()\n",
        "        is_cross_attn = key_value_states is not None\n",
        "        use_rope_for_this_pass = self.use_rope and self.is_self_attention and not is_cross_attn and self.rotary_emb is not None\n",
        "\n",
        "        q = self.q_proj(x)\n",
        "        if is_cross_attn:\n",
        "            T_k = key_value_states.size(1)\n",
        "            k = self.k_proj(key_value_states)\n",
        "            v = self.v_proj(key_value_states)\n",
        "            is_causal = False\n",
        "        else:\n",
        "            T_k = T\n",
        "            k = self.k_proj(x)\n",
        "            v = self.v_proj(x)\n",
        "\n",
        "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, T_k, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, T_k, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        if use_rope_for_this_pass:\n",
        "            cos, sin = self.rotary_emb(T)\n",
        "            q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
        "            scaling_factor = 1.0\n",
        "        else:\n",
        "            scaling_factor = 1.0 / math.sqrt(self.head_dim)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) * scaling_factor\n",
        "\n",
        "        final_mask_bool = None\n",
        "        if attn_mask is not None:\n",
        "            if attn_mask.dim() == 2: padding_mask_bool = ~attn_mask.bool().unsqueeze(1).unsqueeze(2)\n",
        "            elif attn_mask.dim() == 4: padding_mask_bool = ~attn_mask.bool()\n",
        "            else: raise ValueError(f\"Unsupported attn_mask dimension: {attn_mask.dim()}\")\n",
        "            final_mask_bool = padding_mask_bool\n",
        "\n",
        "        if self.is_self_attention and is_causal:\n",
        "            causal_mask_bool = self._get_causal_mask(T, x.device).unsqueeze(0).unsqueeze(0)\n",
        "            if final_mask_bool is not None: final_mask_bool = final_mask_bool | causal_mask_bool\n",
        "            else: final_mask_bool = causal_mask_bool\n",
        "\n",
        "        if final_mask_bool is not None:\n",
        "             scores = scores.masked_fill(final_mask_bool, torch.finfo(scores.dtype).min)\n",
        "\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = self.attn_dropout(attn_weights)\n",
        "        attn_output = torch.matmul(attn_weights, v)\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.out_dropout(self.out_proj(attn_output))\n",
        "\n",
        "# --- 4) Decoder Block ---\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, n_heads, dropout=0.1, max_seq_len=2048):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(embed_dim)\n",
        "        self.self_attention = ImprovedAttention(embed_dim, n_heads, is_self_attention=True, use_rope=True, max_seq_len=max_seq_len)\n",
        "        self.ln2 = nn.LayerNorm(embed_dim)\n",
        "        hidden_dim = 4 * embed_dim\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim), nn.GELU(), nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, embed_dim), nn.Dropout(dropout)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, attention_mask=None, is_causal=True):\n",
        "        residual = x\n",
        "        x_norm = self.ln1(x)\n",
        "        attn_output = self.self_attention(x_norm, attn_mask=attention_mask, is_causal=is_causal)\n",
        "        x = residual + self.dropout(attn_output)\n",
        "        residual = x\n",
        "        x_norm = self.ln2(x)\n",
        "        ff_output = self.feed_forward(x_norm)\n",
        "        x = residual + self.dropout(ff_output)\n",
        "        return x\n",
        "\n",
        "# --- 5) JEPA Predictor Block (Needed for model structure, not used in generation logic) ---\n",
        "class JEPAPredictorBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, n_heads, dropout=0.1, max_seq_len=2048):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(embed_dim)\n",
        "        self.self_attention = ImprovedAttention(embed_dim, n_heads, is_self_attention=True, use_rope=True, max_seq_len=max_seq_len)\n",
        "        self.ln_cross_attn_query = nn.LayerNorm(embed_dim)\n",
        "        self.ln_cross_attn_kv = nn.LayerNorm(embed_dim)\n",
        "        self.cross_attention = ImprovedAttention(embed_dim, n_heads, is_self_attention=False, use_rope=False, max_seq_len=max_seq_len)\n",
        "        self.ln3 = nn.LayerNorm(embed_dim)\n",
        "        hidden_dim = 4 * embed_dim\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim), nn.GELU(), nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, embed_dim), nn.Dropout(dropout)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, decoder_output, self_attention_mask=None, cross_attention_mask=None):\n",
        "        # --- Self-Attention ---\n",
        "        residual = x\n",
        "        attn_output = self.self_attention(self.ln1(x), attn_mask=self_attention_mask, is_causal=True)\n",
        "        x = residual + self.dropout(attn_output)\n",
        "        # --- Cross-Attention ---\n",
        "        residual = x\n",
        "        cross_attn_output = self.cross_attention(\n",
        "            self.ln_cross_attn_query(x),\n",
        "            attn_mask=cross_attention_mask,\n",
        "            key_value_states=self.ln_cross_attn_kv(decoder_output)\n",
        "        )\n",
        "        x = residual + self.dropout(cross_attn_output)\n",
        "        # --- Feed-Forward ---\n",
        "        residual = x\n",
        "        ff_output = self.feed_forward(self.ln3(x))\n",
        "        x = residual + self.dropout(ff_output)\n",
        "        return x\n",
        "\n",
        "# --- 6) Backbone Decoder ---\n",
        "class BackboneDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, n_heads, n_layers, block_size):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            DecoderBlock(embed_dim, n_heads, dropout=0.1, max_seq_len=block_size)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(embed_dim)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear): torch.nn.init.normal_(module.weight, mean=0.0, std=0.02); torch.nn.init.zeros_(module.bias) if module.bias is not None else None\n",
        "        elif isinstance(module, nn.Embedding): torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm): torch.nn.init.zeros_(module.bias); torch.nn.init.ones_(module.weight)\n",
        "\n",
        "    def forward(self, x, attention_mask=None, is_causal=True):\n",
        "        B, T = x.size()\n",
        "        # assert T <= self.block_size, f\"Sequence length {T} exceeds block size {self.block_size}\" # Allow longer during generation cropping\n",
        "        token_emb = self.token_embedding(x)\n",
        "        x = self.dropout(token_emb)\n",
        "        for block in self.blocks: x = block(x, attention_mask=attention_mask, is_causal=is_causal)\n",
        "        x = self.ln_f(x)\n",
        "        return x\n",
        "\n",
        "# --- 7) JEPA Predictor (Needed for model structure, not used in generation logic) ---\n",
        "class JEPAPredictor(nn.Module):\n",
        "    def __init__(self, embed_dim, n_heads, n_layers, block_size):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        predictor_layers = n_layers\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        torch.nn.init.normal_(self.mask_token, mean=0.0, std=0.02)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            JEPAPredictorBlock(embed_dim, n_heads, max_seq_len=block_size)\n",
        "            for _ in range(predictor_layers)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(embed_dim)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear): torch.nn.init.normal_(module.weight, mean=0.0, std=0.02); torch.nn.init.zeros_(module.bias) if module.bias is not None else None\n",
        "        elif isinstance(module, nn.LayerNorm): torch.nn.init.zeros_(module.bias); torch.nn.init.ones_(module.weight)\n",
        "\n",
        "    def forward(self, decoder_output_causal, target_spans_indices, context_mask, attention_mask):\n",
        "        # This forward function is complex and primarily for training JEPA loss.\n",
        "        # It's not directly called during standard autoregressive generation.\n",
        "        # We include the structure for model loading compatibility.\n",
        "        pass # Not needed for SR-ABI inference logic\n",
        "\n",
        "# --- 8) Target Encoder (Needed for model structure, not used in generation logic) ---\n",
        "class TargetEncoder(nn.Module):\n",
        "    def __init__(self, backbone_decoder, ema_decay=0.999):\n",
        "        super().__init__()\n",
        "        self.encoder = copy.deepcopy(backbone_decoder)\n",
        "        self.ema_decay = ema_decay\n",
        "        for param in self.encoder.parameters(): param.requires_grad = False\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_ema(self, backbone_decoder, decay_rate=None): pass # Not needed for inference\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, x, attention_mask=None): pass # Not needed for inference\n",
        "\n",
        "# --- 9) Complete T-JEPA Model ---\n",
        "class TJEPAModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, n_heads, n_layers, block_size, ema_decay=0.999, lm_loss_weight=0.1, pad_token_id=0):\n",
        "        super().__init__()\n",
        "        self.pad_token_id = pad_token_id\n",
        "        self.lm_loss_weight = lm_loss_weight\n",
        "        self.block_size = block_size\n",
        "\n",
        "        self.decoder_backbone = BackboneDecoder(vocab_size, embed_dim, n_heads, n_layers, block_size)\n",
        "        # Predictor and TargetEncoder needed for state_dict loading compatibility\n",
        "        self.predictor = JEPAPredictor(embed_dim, n_heads, n_layers, block_size)\n",
        "        self.target_encoder = TargetEncoder(self.decoder_backbone, ema_decay)\n",
        "\n",
        "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
        "        self.decoder_backbone.token_embedding.weight = self.lm_head.weight # Weight tying\n",
        "\n",
        "    # Forward methods related to training (JEPA loss) are omitted for inference clarity\n",
        "    # We only need the backbone and lm_head for generation.\n",
        "\n",
        "# ==========================================\n",
        "# Helper Functions for Tokenization\n",
        "# ==========================================\n",
        "def _encode(text: str, bos_token: int) -> List[int]:\n",
        "    \"\"\"Encodes text to byte tokens, adding BOS.\"\"\"\n",
        "    return [bos_token] + [b for b in text.encode('utf-8', errors='replace')]\n",
        "\n",
        "def _decode(tokens: List[int], bos_token: int, eos_token: int, pad_token: int) -> str:\n",
        "    \"\"\"Decodes byte tokens to text, removing special tokens.\"\"\"\n",
        "    try:\n",
        "        # Find EOS if present and truncate\n",
        "        eos_pos = tokens.index(eos_token) if eos_token in tokens else -1\n",
        "        if eos_pos != -1:\n",
        "            tokens = tokens[:eos_pos]\n",
        "\n",
        "        # Filter out BOS and PAD, then decode\n",
        "        filtered_bytes = bytes([tok for tok in tokens if tok != bos_token and tok != pad_token])\n",
        "        return filtered_bytes.decode('utf-8', errors='replace')\n",
        "    except Exception as e:\n",
        "        print(f\"Decoding error: {e}\")\n",
        "        return f\"[Decoding Error] Raw bytes: {bytes(tokens)}\"\n",
        "\n",
        "# ==========================================\n",
        "# SR-ABI Inference Function\n",
        "# ==========================================\n",
        "@torch.no_grad()\n",
        "def generate_sr_abi(\n",
        "    model: TJEPAModel,\n",
        "    prompt_text: str,\n",
        "    num_votes: int,           # K\n",
        "    max_new_tokens: int,      # L\n",
        "    temperature: float,       # Part of Theta\n",
        "    top_p: float,             # Part of Theta\n",
        "    hyperparams: Dict[str, Any], # Contains BOS, EOS, PAD IDs etc.\n",
        "    device: str\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generates text using State-Resetting Agreement-Based Inference (SR-ABI).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    bos_token = hyperparams['bos_token']\n",
        "    eos_token = hyperparams['eos_token']\n",
        "    pad_token = hyperparams['pad_token']\n",
        "    block_size = hyperparams['block_size']\n",
        "\n",
        "    # --- Initialization ---\n",
        "    # a. Tokenize prompt (Including system prompt and starting tag)\n",
        "    full_prompt = f\"{hyperparams.get('system_prompt', '')}\\n\\nProblem: {prompt_text}\\n\\n{THINK_TAG_START}\"\n",
        "    prompt_tokens = _encode(full_prompt, bos_token)\n",
        "    # b. Initialize current full sequence S\n",
        "    S_list = prompt_tokens[:] # List of token IDs\n",
        "    # c. Initialize generated sequence G\n",
        "    G_list = [] # List of token IDs (only the generated part)\n",
        "\n",
        "    print(f\"\\n--- Starting SR-ABI Generation (K={num_votes}) ---\")\n",
        "    print(f\"Prompt:\\n{full_prompt}\", end=\"\", flush=True)\n",
        "\n",
        "    # --- Token Generation Loop ---\n",
        "    for i in range(max_new_tokens):\n",
        "        # --- a. Vote Collection ---\n",
        "        votes = Counter()\n",
        "        S_tensor = torch.tensor([S_list], dtype=torch.long, device=device) # Add batch dim [1, T]\n",
        "\n",
        "        # Crop context if it exceeds block size for the forward pass\n",
        "        S_cond = S_tensor if S_tensor.size(1) <= block_size else S_tensor[:, -block_size:]\n",
        "        seq_len = S_cond.size(1)\n",
        "\n",
        "        # Create attention mask for the current sequence\n",
        "        attention_mask = (S_cond != pad_token).float().to(device) # [1, T_cond]\n",
        "\n",
        "        for j in range(num_votes):\n",
        "            # --- i.1 & i.2: Reset State & Re-evaluate Context ---\n",
        "            # This is achieved by running the forward pass on the *full* current\n",
        "            # sequence S_cond. The model calculates state (KV cache) from scratch.\n",
        "            decoder_output = model.decoder_backbone(\n",
        "                S_cond,\n",
        "                attention_mask=attention_mask,\n",
        "                is_causal=True # Standard causal generation\n",
        "            ) # [1, T_cond, C]\n",
        "\n",
        "            # Get logits for the *next* token prediction (using the last token's embedding)\n",
        "            logits = model.lm_head(decoder_output[:, -1, :])  # [1, C] -> [1, V]\n",
        "\n",
        "            # --- i.3: Sample Candidate ---\n",
        "            # Apply temperature\n",
        "            if temperature > 0 and temperature != 1.0:\n",
        "                 logits = logits / temperature\n",
        "\n",
        "            # Apply top-p (nucleus) sampling\n",
        "            if top_p > 0.0 and top_p < 1.0:\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                sorted_indices_to_remove[..., 0] = 0\n",
        "                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "                logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "            # Sample from the filtered distribution\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            candidate_token_tensor = torch.multinomial(probs, num_samples=1) # [1, 1]\n",
        "            candidate_token = candidate_token_tensor.item()\n",
        "\n",
        "            # --- i.4: Record Vote ---\n",
        "            votes[candidate_token] += 1\n",
        "\n",
        "        # --- b. Agreement (Majority Vote) ---\n",
        "        if not votes:\n",
        "            print(\"\\nWarning: No votes collected, stopping generation.\")\n",
        "            break # Should not happen if num_votes >= 1\n",
        "\n",
        "        # Get the token with the most votes. Tie-breaking: implicitly handled by most_common\n",
        "        # (returns items in order of first appearance among ties if counts are equal)\n",
        "        winning_token, vote_count = votes.most_common(1)[0]\n",
        "        # Optional: print vote distribution for debugging\n",
        "        # print(f\"\\nVotes (Step {i+1}): {votes}\")\n",
        "        # print(f\"Winner: {winning_token} ({vote_count}/{num_votes})\")\n",
        "\n",
        "        # --- c. Check for Termination ---\n",
        "        if winning_token == eos_token:\n",
        "            print(\"<EOS>\", flush=True)\n",
        "            break\n",
        "\n",
        "        # --- d. Append Token ---\n",
        "        G_list.append(winning_token)\n",
        "        S_list.append(winning_token)\n",
        "\n",
        "        # --- Print the winning token ---\n",
        "        # Attempt to decode the single winning token for streaming output\n",
        "        try:\n",
        "            print(bytes([winning_token]).decode('utf-8', errors='replace'), end=\"\", flush=True)\n",
        "        except UnicodeDecodeError:\n",
        "            print(\"<?>\", end=\"\", flush=True) # Placeholder for partial UTF-8 chars\n",
        "\n",
        "        # Optional: Small delay for readability\n",
        "        # time.sleep(0.01)\n",
        "\n",
        "    print(\"\\n--- Generation Complete ---\")\n",
        "\n",
        "    # --- Finalization ---\n",
        "    # a. Detokenize the generated sequence G\n",
        "    output_text = _decode(G_list, bos_token, eos_token, pad_token)\n",
        "\n",
        "    return full_prompt + output_text # Return prompt + generated text\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# Model Loading Function\n",
        "# ==========================================\n",
        "def load_model_for_inference(model_path: str, device: str) -> Tuple[Optional[TJEPAModel], Optional[Dict[str, Any]]]:\n",
        "    \"\"\"Loads the TJEPAModel from a checkpoint for inference.\"\"\"\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Error: Model checkpoint not found at {model_path}\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"Loading model checkpoint from {model_path}...\")\n",
        "    try:\n",
        "        checkpoint = torch.load(model_path, map_location=device)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading checkpoint file: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    # Load hyperparams from checkpoint or use defaults\n",
        "    hyperparams_loaded = checkpoint.get('hyperparams', None)\n",
        "    if hyperparams_loaded:\n",
        "        print(\"Using hyperparameters loaded from checkpoint.\")\n",
        "        # Merge with defaults to ensure all needed keys exist\n",
        "        hyperparams = get_default_hyperparams()\n",
        "        hyperparams.update(hyperparams_loaded) # Loaded values override defaults\n",
        "    else:\n",
        "        print(\"Warning: Hyperparameters not found in checkpoint, using default values.\")\n",
        "        hyperparams = get_default_hyperparams()\n",
        "\n",
        "    print(f\"Effective Hyperparameters: {hyperparams}\")\n",
        "\n",
        "    # Create model instance based on loaded/effective hyperparams\n",
        "    try:\n",
        "        model = TJEPAModel(\n",
        "            vocab_size=hyperparams['vocab_size'], embed_dim=hyperparams['embed_dim'],\n",
        "            n_heads=hyperparams['n_heads'], n_layers=hyperparams['n_layers'],\n",
        "            block_size=hyperparams['block_size'], ema_decay=hyperparams['ema_decay'],\n",
        "            lm_loss_weight=hyperparams['lm_loss_weight'], pad_token_id=hyperparams['pad_token']\n",
        "        ).to(device)\n",
        "    except KeyError as e:\n",
        "         print(f\"Error: Missing hyperparameter '{e}' needed to build the model structure.\")\n",
        "         return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating model instance: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    # Load model state dictionary\n",
        "    try:\n",
        "        model_state = checkpoint['model_state']\n",
        "        # Flexible loading\n",
        "        current_model_dict = model.state_dict()\n",
        "        processed_state_dict = {}\n",
        "        warned_keys = set()\n",
        "        loaded_keys_count = 0\n",
        "        for k, v in model_state.items():\n",
        "            new_k = k # Handle potential renames if needed in the future\n",
        "            # Example rename: if k.startswith(\"old_prefix.\"): new_k = k.replace(\"old_prefix.\", \"new_prefix.\", 1)\n",
        "            if new_k in current_model_dict:\n",
        "                if v.shape == current_model_dict[new_k].shape:\n",
        "                    processed_state_dict[new_k] = v\n",
        "                    loaded_keys_count += 1\n",
        "                else:\n",
        "                    if new_k not in warned_keys:\n",
        "                        print(f\"Warning: Shape mismatch for key '{new_k}'. Checkpoint: {v.shape}, Model: {current_model_dict[new_k].shape}. Skipping.\")\n",
        "                        warned_keys.add(new_k)\n",
        "            # else:\n",
        "            #     if k not in warned_keys and new_k not in warned_keys:\n",
        "            #         print(f\"Warning: Key '{k}' (mapped to '{new_k}') not found in current model. Skipping.\")\n",
        "            #         warned_keys.add(k); warned_keys.add(new_k)\n",
        "\n",
        "        missing_keys, unexpected_keys = model.load_state_dict(processed_state_dict, strict=False)\n",
        "        if missing_keys: print(f\"  Info: Missing keys in final state_dict load: {missing_keys}\")\n",
        "        if unexpected_keys: print(f\"  Info: Unexpected keys found in checkpoint but not used: {unexpected_keys}\")\n",
        "        print(f\"Model state loaded successfully ({loaded_keys_count} tensors loaded).\")\n",
        "        loaded_epoch = checkpoint.get('epoch', -1); loaded_step = checkpoint.get('current_step', -1)\n",
        "        val_loss = checkpoint.get('val_loss', 'N/A')\n",
        "        val_loss_str = f\"{val_loss:.4f}\" if isinstance(val_loss, float) else str(val_loss)\n",
        "        print(f\"  Checkpoint details: Epoch {loaded_epoch}, Step {loaded_step}, Val Loss {val_loss_str}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model state weights: {e}\")\n",
        "        print(\"Attempting inference with potentially uninitialized weights.\")\n",
        "\n",
        "    return model, hyperparams\n",
        "\n",
        "# ==========================================\n",
        "# Main Execution Block\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Setup Device ---\n",
        "    device = \"mps\" if torch.backends.mps.is_available() else \\\n",
        "             (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # --- Load Model ---\n",
        "    model, hyperparams = load_model_for_inference(MODEL_PATH, device)\n",
        "\n",
        "    if model and hyperparams:\n",
        "        # --- Run SR-ABI Generation ---\n",
        "        start_time = time.time()\n",
        "        generated_text = generate_sr_abi(\n",
        "            model=model,\n",
        "            prompt_text=PROMPT_TEXT,\n",
        "            num_votes=NUM_VOTES,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            temperature=TEMPERATURE,\n",
        "            top_p=TOP_P,\n",
        "            hyperparams=hyperparams,\n",
        "            device=device\n",
        "        )\n",
        "        end_time = time.time()\n",
        "\n",
        "        print(\"\\n\\n--- Final Output ---\")\n",
        "        print(generated_text)\n",
        "        print(f\"\\nGeneration took {end_time - start_time:.2f} seconds.\")\n",
        "    else:\n",
        "        print(\"Failed to load model. Exiting.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j43CB3fXJgC6",
        "outputId": "9f1683a6-83b1-457e-aed0-fcf4cd0e0e70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading model checkpoint from t_jepa_mtl_decoder_rope_bs1024_checkpoint.pt...\n",
            "Using hyperparameters loaded from checkpoint.\n",
            "Effective Hyperparameters: {'vocab_size': 256, 'embed_dim': 512, 'n_heads': 8, 'n_layers': 12, 'block_size': 1024, 'ema_decay': 0.999, 'lm_loss_weight': 0.92, 'bos_token': 254, 'eos_token': 255, 'pad_token': 0, 'top_p': 0.8, 'context_span_ratio': 0.6, 'target_span_ratio': 0.2, 'num_target_spans': 8, 'min_span_length': 32, 'thinking_tag': '<think>', 'thinking_end_tag': '</think>', 'answer_tag': '<answer>', 'answer_end_tag': '</answer>', 'system_prompt': 'Consider this math problem. Think step by step and provide your reasoning between <think> </think> tags, then give your final answer between <answer> </answer> tags.', 'batch_size': 2, 'num_epochs': 50, 'steps_per_epoch': 1000, 'eval_interval': 200, 'eval_iters': 100, 'accumulation_steps': 8, 'generate_num_tokens': 1024, 'start_prompt': 'Problem: A bakery produces cakes for $10 each. It costs them $5 in ingredients per cake, and they have a fixed overhead of $200 per day. How many cakes do they need to sell each day to make a daily profit of $100?', 'checkpoint_path': 't_jepa_mtl_decoder_rope_bs1024_checkpoint.pt', 'continue_training': True}\n",
            "Model state loaded successfully (586 tensors loaded).\n",
            "  Checkpoint details: Epoch 49, Step 50000, Val Loss tensor(0.9010, device='cuda:0')\n",
            "\n",
            "--- Starting SR-ABI Generation (K=8) ---\n",
            "Prompt:\n",
            "Consider this math problem. Think step by step and provide your reasoning between <think> </think> tags, then give your final answer between <answer> </answer> tags.\n",
            "\n",
            "Problem: what is five plus two?\n",
            "\n",
            "<think>Let N be the number of plus the same price. The number of plus that was left is 2 so 2 so 2*2 = <<2*2=4>>4 so the plus two plus the price.\n",
            "Since the plus 2 plus the price of the plus the plus the price, the price what was 4*2 = <<4*2=8>>8 so the plus two plus the plus 2.\n",
            "#### 8</think>\n",
            "\n",
            "<answer>8</answer><EOS>\n",
            "\n",
            "--- Generation Complete ---\n",
            "\n",
            "\n",
            "--- Final Output ---\n",
            "Consider this math problem. Think step by step and provide your reasoning between <think> </think> tags, then give your final answer between <answer> </answer> tags.\n",
            "\n",
            "Problem: what is five plus two?\n",
            "\n",
            "<think>Let N be the number of plus the same price. The number of plus that was left is 2 so 2 so 2*2 = <<2*2=4>>4 so the plus two plus the price.\n",
            "Since the plus 2 plus the price of the plus the plus the price, the price what was 4*2 = <<4*2=8>>8 so the plus two plus the plus 2.\n",
            "#### 8</think>\n",
            "\n",
            "<answer>8</answer>\n",
            "\n",
            "Generation took 41.39 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XfSort-iGeAG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}